"""
Stock Information Extractor
Extract stock names and numbers from conversations using LLMs with Pydantic structured output
"""

import traceback
from typing import List, Optional
import gradio as gr
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import PydanticOutputParser


# ============================================================================
# Pydantic Models for Structured Output
# ============================================================================

class StockInfo(BaseModel):
    """Information about a single stock mentioned in the conversation"""
    stock_number: str = Field(
        description="The stock code/number (e.g., '00700', '1810', '18138')"
    )
    stock_name: str = Field(
        description="The stock name in Traditional Chinese (e.g., 'é¨°è¨Š', 'å°ç±³', 'æ‹›å•†å±€ç½®åœ°')"
    )
    confidence: str = Field(
        description="Confidence level: 'high', 'medium', or 'low'"
    )
    reasoning: Optional[str] = Field(
        default=None,
        description="Brief explanation of how the stock was identified or any corrections made"
    )


class ConversationStockExtraction(BaseModel):
    """Complete extraction result from a conversation"""
    stocks: List[StockInfo] = Field(
        description="List of all stocks mentioned in the conversation"
    )
    summary: str = Field(
        description="Brief summary of the conversation context"
    )


# ============================================================================
# Model Configuration
# ============================================================================

MODEL_OPTIONS = [
    "qwen3:32b",
    "gpt-oss:20b",
    "gemma3-27b",
    "deepseek-r1:32b",
    "deepseek-r1:70b",
    "qwen2.5:72b",
    "llama3.3:70b",
]

DEFAULT_MODEL = MODEL_OPTIONS[0]
DEFAULT_OLLAMA_URL = "http://localhost:11434"

DEFAULT_SYSTEM_MESSAGE = """ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªçš„é¦™æ¸¯è‚¡å¸‚åˆ†æå°ˆå®¶ã€‚ä½ çš„ä»»å‹™æ˜¯å¾é›»è©±éŒ„éŸ³çš„æ–‡å­—è½‰éŒ„ä¸­è­˜åˆ¥æ‰€æœ‰æåŠçš„è‚¡ç¥¨ã€‚

**é‡è¦æç¤º:**
- ç”±æ–¼Speech-to-TextæŠ€è¡“çš„èª¤å·®ï¼Œæ–‡å­—ä¸­å¯èƒ½æœ‰èª¤èªè©å½™
- ä½ éœ€è¦é‹ç”¨å°ˆæ¥­çŸ¥è­˜å’Œé‚è¼¯æ¨ç†ï¼Œæ¨æ–·ä¸¦é‚„åŸæ­£ç¢ºçš„è‚¡ç¥¨è³‡è¨Š
- è‚¡ç¥¨ä»£è™Ÿå¯èƒ½ä»¥ä¸åŒå½¢å¼å‡ºç¾ï¼ˆä¾‹å¦‚ï¼šã€Œä¸ƒç™¾ã€å¯èƒ½æ˜¯ã€Œ00700ã€é¨°è¨Šï¼‰

**å¸¸è¦‹èª¤å·®:**
- èª¤èª: ç™¾ â†’ æ­£ç¢º: å…« (ä¾‹: ä¸€ç™¾ä¸€ä¸‰å…« â†’ 18138)
- èª¤èª: å­¤/æ²½ â†’ æ­£ç¢º: è³£å‡º
- èª¤èª: è½® â†’ æ­£ç¢º: çª©è¼ª

**ä½ çš„ç›®æ¨™:**
1. è­˜åˆ¥æ‰€æœ‰æåŠçš„è‚¡ç¥¨ä»£è™Ÿå’Œåç¨±
2. ä¿®æ­£ä»»ä½•å¯èƒ½çš„Speech-to-Textèª¤å·®
3. è©•ä¼°æ¯å€‹è­˜åˆ¥çš„ç½®ä¿¡åº¦ï¼ˆhigh/medium/lowï¼‰
4. æä¾›ç°¡è¦çš„æ¨ç†è§£é‡‹

è«‹ä»¥çµæ§‹åŒ–çš„JSONæ ¼å¼è¿”å›çµæœã€‚"""


# ============================================================================
# Core Extraction Function
# ============================================================================

def extract_stocks_from_conversation(
    conversation_text: str,
    model: str,
    ollama_url: str,
    system_message: str,
    temperature: float,
) -> tuple[str, str]:
    """
    Extract stock information from conversation using LLM with Pydantic structured output
    
    Args:
        conversation_text: The conversation transcript
        model: LLM model name
        ollama_url: Ollama server URL
        system_message: System message for the LLM
        temperature: Temperature for generation
    
    Returns:
        tuple[str, str]: (formatted_result, raw_json)
    """
    try:
        # Validate inputs
        if not conversation_text or not conversation_text.strip():
            return "âŒ Error: Please provide conversation text", ""
        
        if not model or not model.strip():
            return "âŒ Error: Please specify a model name", ""
        
        if not ollama_url or not ollama_url.strip():
            return "âŒ Error: Please specify Ollama URL", ""
        
        # Initialize parser and LLM
        parser = PydanticOutputParser(pydantic_object=ConversationStockExtraction)
        
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
        )
        
        # Build the complete prompt with format instructions
        format_instructions = parser.get_format_instructions()
        
        full_prompt = f"""{conversation_text}

{format_instructions}

è«‹æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¿”å›æ‰€æœ‰è­˜åˆ¥å‡ºçš„è‚¡ç¥¨è³‡è¨Šã€‚"""
        
        # Prepare messages
        messages = [
            ("system", system_message),
            ("human", full_prompt),
        ]
        
        # Get response from LLM
        print(f"ğŸ”„ Calling {model}...")
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        print(f"ğŸ“¥ Raw response received (length: {len(response_content)} chars)")
        
        # Parse the response
        try:
            parsed_result: ConversationStockExtraction = parser.parse(response_content)
            
            # Format the result for display
            formatted_output = format_extraction_result(parsed_result)
            
            # Also return the raw JSON for reference
            raw_json = parsed_result.model_dump_json(indent=2, exclude_none=True)
            
            return formatted_output, raw_json
            
        except Exception as parse_error:
            error_msg = f"âš ï¸ Warning: Could not parse structured output\n\n"
            error_msg += f"Parse Error: {str(parse_error)}\n\n"
            error_msg += f"Raw LLM Response:\n{response_content}"
            return error_msg, response_content
    
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return error_msg, ""


def format_extraction_result(result: ConversationStockExtraction) -> str:
    """Format the extraction result for display"""
    output = []
    
    output.append("=" * 80)
    output.append("ğŸ“Š è‚¡ç¥¨æå–çµæœ (Stock Extraction Results)")
    output.append("=" * 80)
    output.append("")
    
    # Summary
    output.append(f"ğŸ“ å°è©±æ‘˜è¦: {result.summary}")
    output.append("")
    
    # Stocks found
    output.append(f"ğŸ” æ‰¾åˆ° {len(result.stocks)} å€‹è‚¡ç¥¨:")
    output.append("")
    
    if not result.stocks:
        output.append("   âš ï¸ æœªæ‰¾åˆ°ä»»ä½•è‚¡ç¥¨è³‡è¨Š")
    else:
        for i, stock in enumerate(result.stocks, 1):
            confidence_emoji = {
                "high": "âœ…",
                "medium": "âš¡",
                "low": "âš ï¸"
            }.get(stock.confidence.lower(), "â“")
            
            output.append(f"   {i}. {confidence_emoji} è‚¡ç¥¨ #{i}")
            output.append(f"      â€¢ è‚¡ç¥¨ä»£è™Ÿ: {stock.stock_number}")
            output.append(f"      â€¢ è‚¡ç¥¨åç¨±: {stock.stock_name}")
            output.append(f"      â€¢ ç½®ä¿¡åº¦: {stock.confidence.upper()}")
            
            if stock.reasoning:
                output.append(f"      â€¢ æ¨ç†: {stock.reasoning}")
            
            output.append("")
    
    output.append("=" * 80)
    
    return "\n".join(output)


# ============================================================================
# Gradio Interface
# ============================================================================

def create_stock_extractor_interface():
    """Create the Gradio interface for stock extraction"""
    
    with gr.Blocks(title="Stock Extractor", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ“ˆ æ™ºèƒ½è‚¡ç¥¨ä¿¡æ¯æå–å™¨
            ## Stock Information Extractor with Pydantic & LLM
            
            å¾å°è©±è¨˜éŒ„ä¸­è‡ªå‹•è­˜åˆ¥å’Œæå–è‚¡ç¥¨ä»£è™Ÿå’Œåç¨±ï¼Œä½¿ç”¨çµæ§‹åŒ–è¼¸å‡ºä¿è­‰æ•¸æ“šè³ªé‡ã€‚
            """
        )
        
        with gr.Row():
            # Left column - Input
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“ è¼¸å…¥è¨­ç½® (Input Settings)")
                
                conversation_input = gr.Textbox(
                    label="å°è©±è¨˜éŒ„ (Conversation Text)",
                    placeholder="è«‹è¼¸å…¥å°è©±å…§å®¹...\n\nä¾‹å¦‚ï¼š\nåˆ¸å•†ï¼šä½ å¥½ï¼Œè«‹å•éœ€è¦ä»€éº¼å¹«åŠ©ï¼Ÿ\nå®¢æˆ¶ï¼šæˆ‘æƒ³è²·é¨°è¨Š\nåˆ¸å•†ï¼šå¥½çš„ï¼Œä¸ƒç™¾è™Ÿï¼Œè²·å¤šå°‘ï¼Ÿ\nå®¢æˆ¶ï¼šä¸€åƒè‚¡...",
                    lines=12,
                )
                
                gr.Markdown("### âš™ï¸ LLM æ¨¡å‹è¨­ç½® (LLM Settings)")
                
                model_dropdown = gr.Dropdown(
                    choices=MODEL_OPTIONS,
                    value=DEFAULT_MODEL,
                    label="é¸æ“‡æ¨¡å‹ (Select Model)",
                    allow_custom_value=True,
                )
                
                with gr.Row():
                    ollama_url_input = gr.Textbox(
                        label="Ollama URL",
                        value=DEFAULT_OLLAMA_URL,
                        scale=3,
                    )
                    
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.1,
                        step=0.1,
                        label="Temperature",
                        scale=1,
                    )
                
                system_message_input = gr.Textbox(
                    label="ç³»çµ±è¨Šæ¯ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=8,
                )
                
                extract_btn = gr.Button(
                    "ğŸš€ é–‹å§‹æå–è‚¡ç¥¨è³‡è¨Š",
                    variant="primary",
                    size="lg",
                )
            
            # Right column - Output
            with gr.Column(scale=1):
                gr.Markdown("### ğŸ“Š æå–çµæœ (Extraction Results)")
                
                result_output = gr.Textbox(
                    label="æ ¼å¼åŒ–çµæœ (Formatted Results)",
                    lines=15,
                    interactive=False,
                )
                
                gr.Markdown("### ğŸ”§ åŸå§‹ JSON è¼¸å‡º (Raw JSON Output)")
                
                json_output = gr.Textbox(
                    label="çµæ§‹åŒ–æ•¸æ“š (Structured Data)",
                    lines=12,
                    interactive=False,
                    show_copy_button=True,
                )
        
        # Example conversations
        gr.Markdown("### ğŸ’¡ ç¤ºä¾‹å°è©± (Example Conversations)")
        
        with gr.Row():
            example_1 = gr.Button("ç¤ºä¾‹ 1: é¨°è¨Šäº¤æ˜“", size="sm")
            example_2 = gr.Button("ç¤ºä¾‹ 2: å¤šéš»è‚¡ç¥¨è¨è«–", size="sm")
            example_3 = gr.Button("ç¤ºä¾‹ 3: èªéŸ³è­˜åˆ¥èª¤å·®", size="sm")
        
        # Event handlers
        extract_btn.click(
            fn=extract_stocks_from_conversation,
            inputs=[
                conversation_input,
                model_dropdown,
                ollama_url_input,
                system_message_input,
                temperature_slider,
            ],
            outputs=[result_output, json_output],
        )
        
        # Example button handlers
        example_1.click(
            fn=lambda: """åˆ¸å•†ï¼šä½ å¥½ï¼Œè«‹å•éœ€è¦ä»€éº¼å¹«åŠ©ï¼Ÿ
å®¢æˆ¶ï¼šæˆ‘æƒ³è²·é¨°è¨Š
åˆ¸å•†ï¼šå¥½çš„ï¼Œä¸ƒç™¾è™Ÿé¨°è¨Šï¼Œè²·å¤šå°‘ï¼Ÿ
å®¢æˆ¶ï¼šä¸€åƒè‚¡ï¼Œå¸‚åƒ¹è²·å…¥
åˆ¸å•†ï¼šç¢ºèªä¸€ä¸‹ï¼Œä¸ƒç™¾è™Ÿé¨°è¨Šï¼Œè²·å…¥ä¸€åƒè‚¡ï¼Œå¸‚åƒ¹ï¼Œå°å—ï¼Ÿ
å®¢æˆ¶ï¼šå°çš„ï¼Œè¬è¬""",
            outputs=[conversation_input],
        )
        
        example_2.click(
            fn=lambda: """å®¢æˆ¶ï¼šæ—©æ™¨ï¼Œæˆ‘æƒ³å•ä¸‹å°ç±³åŒæ¯”äºè¿ªä»Šæ—¥èµ°å‹¢
åˆ¸å•†ï¼šä½ å¥½ï¼å°ç±³ä¸€å…«ä¸€é›¶ä»Šæ—¥å‡å’—2%ï¼Œæ¯”äºè¿ªäºŒä¸€ä¸€ä¸€è·Œå’—1%
å®¢æˆ¶ï¼šå’æˆ‘æƒ³æ²½äº”ç™¾è‚¡æ¯”äºè¿ªï¼Œå†å…¥ä¸€åƒè‚¡å°ç±³
åˆ¸å•†ï¼šå¥½çš„ï¼Œç¢ºèªä¸€ä¸‹ï¼šæ²½å‡ºæ¯”äºè¿ªäºŒä¸€ä¸€ä¸€äº”ç™¾è‚¡ï¼Œè²·å…¥å°ç±³ä¸€å…«ä¸€é›¶ä¸€åƒè‚¡ï¼Œå•±å””å•±ï¼Ÿ
å®¢æˆ¶ï¼šå•±ï¼Œå°±å’åš""",
            outputs=[conversation_input],
        )
        
        example_3.click(
            fn=lambda: """å®¢æˆ¶ï¼šæˆ‘æƒ³è²·æ‹›å•†å±€ç½®åœ°
åˆ¸å•†ï¼šæ‹›å•†å±€ç½®åœ°ï¼Œä¿‚ä¸€ç™¾ä¸€ä¸‰å…«è™Ÿï¼Ÿ
å®¢æˆ¶ï¼šä¿‚å‘€
åˆ¸å•†ï¼šè²·å¹¾å¤šï¼Ÿ
å®¢æˆ¶ï¼šäº”ç™¾è‚¡
åˆ¸å•†ï¼šç¢ºèªï¼šä¸€ç™¾ä¸€ä¸‰å…«è™Ÿæ‹›å•†å±€ç½®åœ°ï¼Œè²·å…¥äº”ç™¾è‚¡
å®¢æˆ¶ï¼šæ­£ç¢º

è¨»ï¼šé€™è£¡ã€Œä¸€ç™¾ä¸€ä¸‰å…«ã€æ˜¯èªéŸ³è­˜åˆ¥éŒ¯èª¤ï¼Œæ‡‰è©²æ˜¯ã€Œä¸€å…«ä¸€ä¸‰å…«ã€(18138)""",
            outputs=[conversation_input],
        )
        
        gr.Markdown(
            """
            ---
            ### ğŸ“Œ ä½¿ç”¨èªªæ˜ (Instructions)
            
            1. **è¼¸å…¥å°è©±**: åœ¨å·¦å´æ–‡æœ¬æ¡†ä¸­è¼¸å…¥æˆ–è²¼ä¸Šå°è©±è¨˜éŒ„
            2. **é¸æ“‡æ¨¡å‹**: å¾ä¸‹æ‹‰èœå–®é¸æ“‡ LLM æ¨¡å‹ï¼ˆå»ºè­°ä½¿ç”¨ qwen3:32b æˆ– deepseek-r1:32bï¼‰
            3. **èª¿æ•´åƒæ•¸**: å¯é¸èª¿æ•´ Temperatureï¼ˆå»ºè­° 0.1-0.3 ä»¥ç²å¾—æ›´ç©©å®šçš„çµæœï¼‰
            4. **é–‹å§‹æå–**: é»æ“Šã€Œé–‹å§‹æå–è‚¡ç¥¨è³‡è¨Šã€æŒ‰éˆ•
            5. **æŸ¥çœ‹çµæœ**: å³å´æœƒé¡¯ç¤ºæ ¼å¼åŒ–çš„çµæœå’ŒåŸå§‹ JSON æ•¸æ“š
            
            ### ğŸ¯ åŠŸèƒ½ç‰¹é» (Features)
            
            - âœ… **çµæ§‹åŒ–è¼¸å‡º**: ä½¿ç”¨ Pydantic ä¿è­‰æ•¸æ“šæ ¼å¼ä¸€è‡´
            - ğŸ” **æ™ºèƒ½è­˜åˆ¥**: è‡ªå‹•è­˜åˆ¥è‚¡ç¥¨ä»£è™Ÿå’Œåç¨±
            - ğŸ› ï¸ **èª¤å·®ä¿®æ­£**: å¯ä»¥è­˜åˆ¥ä¸¦ä¿®æ­£ Speech-to-Text éŒ¯èª¤
            - ğŸ“Š **ç½®ä¿¡åº¦è©•ä¼°**: æ¯å€‹è­˜åˆ¥çµæœéƒ½æœ‰ç½®ä¿¡åº¦è©•åˆ†
            - ğŸ”„ **å¤šæ¨¡å‹æ”¯æŒ**: å¯ä»¥é¸æ“‡ä¸åŒçš„ LLM æ¨¡å‹é€²è¡Œæ¯”è¼ƒ
            """
        )
    
    return demo


# ============================================================================
# Main Entry Point
# ============================================================================

def main():
    """Launch the Gradio interface"""
    demo = create_stock_extractor_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
    )


if __name__ == "__main__":
    main()

