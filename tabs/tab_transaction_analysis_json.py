"""
Tab: Transaction Analysis with JSON Stock Reference
Analyze two transcriptions to identify stock transactions using merged JSON stock data
"""

import json
import traceback
from typing import Literal, Optional
import gradio as gr

from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from opencc import OpenCC

# Import centralized model configuration
from model_config import MODEL_OPTIONS, DEFAULT_MODEL, DEFAULT_OLLAMA_URL

# Import from the stock comparison tab for conversation analysis
from tabs.tab_stt_stock_comparison import (
    extract_stocks_with_single_llm,
    DEFAULT_SYSTEM_MESSAGE as STOCK_EXTRACTION_SYSTEM_MESSAGE,
    LLM_OPTIONS,
)


# ============================================================================
# OpenCC Translation Setup
# ============================================================================

# Initialize OpenCC converter (Simplified to Traditional Chinese)
opencc_converter = OpenCC('s2t')  # s2t = Simplified to Traditional

def translate_to_traditional_chinese(text: str) -> str:
    """
    Convert Simplified Chinese text to Traditional Chinese using OpenCC.
    
    Args:
        text: Input text (may contain Simplified Chinese)
        
    Returns:
        str: Text with Simplified Chinese converted to Traditional Chinese
    """
    if not text or not text.strip():
        return text
    
    try:
        return opencc_converter.convert(text)
    except Exception as e:
        print(f"OpenCC translation failed: {e}")
        return text  # Return original text if translation fails


# ============================================================================
# Pydantic Models
# ============================================================================

# Pydantic models for structured transaction output
class Transaction(BaseModel):
    """Represents a single transaction"""
    
    transaction_type: Literal["buy", "sell", "queue"] = Field(
        description="The type of transaction identified: buy, sell, or queue"
    )
    
    confidence_score: float = Field(
        ge=0.0, 
        le=1.0,
        description="Confidence score from 0 to 1. 0=not sure at all, 0.5=moderately confident, 1.0=very confident"
    )
    
    conversation_number: Optional[int] = Field(
        default=None,
        description="The conversation number this transaction came from"
    )
    
    hkt_datetime: Optional[str] = Field(
        default=None,
        description="The Hong Kong datetime when the conversation/transaction occurred"
    )
    
    broker_id: Optional[str] = Field(
        default=None,
        description="The broker ID from the conversation metadata"
    )
    
    broker_name: Optional[str] = Field(
        default=None,
        description="The broker name from the conversation metadata"
    )
    
    client_id: Optional[str] = Field(
        default=None,
        description="The client ID from the conversation metadata"
    )
    
    client_name: Optional[str] = Field(
        default=None,
        description="The client name from the conversation metadata"
    )
    
    stock_code: Optional[str] = Field(
        default=None,
        description="The stock code/number identified in the conversation"
    )
    
    stock_name: Optional[str] = Field(
        default=None,
        description="The stock name identified in the conversation"
    )
    
    quantity: Optional[str] = Field(
        default=None,
        description="The quantity/amount of stocks in the transaction"
    )
    
    price: Optional[str] = Field(
        default=None,
        description="The price mentioned in the transaction"
    )
    
    explanation: str = Field(
        description="Detailed explanation of why this transaction type and confidence score were assigned"
    )


class TransactionAnalysisResult(BaseModel):
    """Complete analysis result with multiple transactions"""
    
    transactions: list[Transaction] = Field(
        default_factory=list,
        description="List of all transactions identified in the conversation. Empty list if no transactions found."
    )
    
    transcription_comparison: str = Field(
        description="Comparison of the two transcriptions and how they differ"
    )
    
    overall_summary: str = Field(
        description="Overall summary of the conversation and all transactions identified"
    )


def process_conversation_json_to_merged(
    conversation_json_input: str,
    selected_llms: list[str],
    ollama_url: str,
    temperature: float,
    use_vector_correction: bool = True,
) -> tuple[str, str]:
    """
    Process conversation JSON input and convert to merged JSON format
    
    Args:
        conversation_json_input: JSON string with conversation data
        selected_llms: List of LLM models to use for stock extraction
        ollama_url: Ollama server URL
        temperature: Temperature parameter
        use_vector_correction: Whether to use vector store correction
        
    Returns:
        tuple: (status_message, merged_json_output)
    """
    try:
        # Parse conversation JSON
        try:
            conversations = json.loads(conversation_json_input)
        except json.JSONDecodeError as e:
            return (f"âŒ éŒ¯èª¤ï¼šç„¡æ•ˆçš„JSONæ ¼å¼\n\n{str(e)}", "")
        
        # Validate it's a list
        if not isinstance(conversations, list):
            return ("âŒ éŒ¯èª¤ï¼šJSONå¿…é ˆæ˜¯å°è©±å°è±¡çš„æ•¸çµ„", "")
        
        if len(conversations) == 0:
            return ("âŒ éŒ¯èª¤ï¼šJSONæ•¸çµ„ç‚ºç©º", "")
        
        # Collect all stocks from all conversations
        all_stocks = []
        status_parts = []
        
        status_parts.append(f"ğŸ”„ è™•ç† {len(conversations)} å€‹å°è©±...")
        status_parts.append(f"ğŸ“Š ä½¿ç”¨ {len(selected_llms)} å€‹LLMæ¨¡å‹: {', '.join(selected_llms)}")
        status_parts.append("")
        
        for conv_idx, conversation in enumerate(conversations, 1):
            conv_number = conversation.get("conversation_number", conv_idx)
            transcriptions = conversation.get("transcriptions", {})
            
            # Get transcription text
            transcription_text = None
            transcription_source = None
            
            if isinstance(transcriptions, dict):
                for source_name, text in transcriptions.items():
                    if text and text.strip():
                        transcription_text = text
                        transcription_source = source_name
                        break
            elif isinstance(transcriptions, str):
                transcription_text = transcriptions
                transcription_source = "default"
            
            if not transcription_text or not transcription_text.strip():
                status_parts.append(f"âš ï¸ è·³éå°è©± #{conv_number} - ç„¡è½‰éŒ„æ–‡å­—")
                continue
            
            status_parts.append(f"ğŸ“ è™•ç†å°è©± #{conv_number}...")
            
            # Extract stocks using each LLM
            for llm_model in selected_llms:
                result_model, formatted_result, raw_json = extract_stocks_with_single_llm(
                    model=llm_model,
                    conversation_text=transcription_text,
                    system_message=STOCK_EXTRACTION_SYSTEM_MESSAGE,
                    ollama_url=ollama_url,
                    temperature=temperature,
                    stt_source=transcription_source,
                    use_vector_correction=use_vector_correction
                )
                
                # Parse and collect stocks
                if raw_json and raw_json.strip():
                    try:
                        parsed = json.loads(raw_json)
                        stocks = parsed.get("stocks", [])
                        for stock in stocks:
                            stock["llm_model"] = llm_model
                            stock["conversation_number"] = conv_number
                        all_stocks.extend(stocks)
                        status_parts.append(f"  âœ“ {llm_model}: æ‰¾åˆ° {len(stocks)} å€‹è‚¡ç¥¨")
                    except json.JSONDecodeError:
                        status_parts.append(f"  âš ï¸ {llm_model}: ç„¡æ³•è§£æè¼¸å‡º")
            
            status_parts.append("")
        
        # Merge and deduplicate stocks (similar to create_merged_stocks_json)
        stocks_dict = {}
        total_analyses = len(conversations) * len(selected_llms)
        
        for stock in all_stocks:
            stock_number = stock.get("stock_number", "")
            if stock_number:
                if stock_number not in stocks_dict:
                    stocks_dict[stock_number] = []
                stocks_dict[stock_number].append(stock)
        
        # Create merged stocks list
        merged_stocks = []
        for stock_number, stock_list in stocks_dict.items():
            if not stock_list:
                continue
            
            # Calculate average relevance_score
            relevance_scores = [s.get("relevance_score", 0) for s in stock_list]
            total_score = sum(relevance_scores)
            avg_relevance_score = total_score / total_analyses if total_analyses > 0 else 0
            
            # Use first stock's data as base
            merged_stock = {
                "stock_number": stock_number,
                "stock_name": stock_list[0].get("stock_name", ""),
                "relevance_score": round(avg_relevance_score, 2),
            }
            
            # Include original_word if present
            original_words = [s.get("original_word", "") for s in stock_list if s.get("original_word")]
            if original_words:
                from collections import Counter
                word_counts = Counter(original_words)
                merged_stock["original_word"] = word_counts.most_common(1)[0][0]
            
            # Include quantity and price if present
            quantities = [s.get("quantity", "") for s in stock_list if s.get("quantity")]
            if quantities:
                from collections import Counter
                qty_counts = Counter(quantities)
                merged_stock["quantity"] = qty_counts.most_common(1)[0][0]
            
            prices = [s.get("price", "") for s in stock_list if s.get("price")]
            if prices:
                from collections import Counter
                price_counts = Counter(prices)
                merged_stock["price"] = price_counts.most_common(1)[0][0]
            
            # Include corrected stock information
            corrected_names = [s.get("corrected_stock_name") for s in stock_list if s.get("corrected_stock_name")]
            corrected_numbers = [s.get("corrected_stock_number") for s in stock_list if s.get("corrected_stock_number")]
            correction_confidences = [s.get("correction_confidence") for s in stock_list if s.get("correction_confidence")]
            
            merged_stock["corrected_stock_number"] = corrected_numbers[0] if corrected_numbers else stock_number
            merged_stock["corrected_stock_name"] = corrected_names[0] if corrected_names else stock_list[0].get("stock_name", "")
            merged_stock["correction_confidence"] = correction_confidences[0] if correction_confidences else 1.0
            
            # Confidence
            confidences = [s.get("confidence", "low").lower() for s in stock_list]
            confidence_priority = {"high": 3, "medium": 2, "low": 1}
            most_confident = max(confidences, key=lambda c: (confidences.count(c), confidence_priority.get(c, 0)))
            merged_stock["confidence"] = most_confident
            
            # Detection count
            merged_stock["detection_count"] = len(stock_list)
            
            # Track which LLM models detected this stock
            llm_models = [s.get("llm_model", "") for s in stock_list if s.get("llm_model")]
            if llm_models:
                unique_models = list(dict.fromkeys(llm_models))
                merged_stock["detected_by_llms"] = unique_models
            
            merged_stocks.append(merged_stock)
        
        # Sort by relevance_score
        merged_stocks.sort(key=lambda s: (-s["relevance_score"], s["stock_number"]))
        
        # Create merged data
        merged_data = {
            "stocks": merged_stocks,
            "metadata": {
                "total_conversations": len(conversations),
                "total_analyses": total_analyses,
                "unique_stocks_found": len(merged_stocks),
                "note": "å¾å°è©±JSONè‡ªå‹•æå–å’Œåˆä½µçš„è‚¡ç¥¨æ•¸æ“š"
            }
        }
        
        merged_json = json.dumps(merged_data, indent=2, ensure_ascii=False)
        
        status_parts.append(f"âœ… å®Œæˆï¼æ‰¾åˆ° {len(merged_stocks)} å€‹å”¯ä¸€è‚¡ç¥¨")
        status_message = "\n".join(status_parts)
        
        return (status_message, merged_json)
        
    except Exception as e:
        error_msg = f"âŒ éŒ¯èª¤: {str(e)}\n\n{traceback.format_exc()}"
        return (error_msg, "")


DEFAULT_SYSTEM_MESSAGE = """ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªçš„é¦™æ¸¯è‚¡å¸‚åˆ†æå¸«ï¼Œå°ˆé–€åˆ†æå°è©±è½‰éŒ„ä¸¦è­˜åˆ¥æ½›åœ¨çš„è‚¡ç¥¨äº¤æ˜“ã€‚

ä½ çš„ä»»å‹™æ˜¯ï¼š
1. **æ ¸å¿ƒä»»å‹™ï¼šä»”ç´°åˆ†æå°è©±è½‰éŒ„å…§å®¹ï¼ˆä¸»è¦è³‡æ–™ä¾†æºï¼‰**
2. **åƒè€ƒè‚¡ç¥¨åƒè€ƒè³‡æ–™ï¼ˆæ¬¡è¦è³‡æ–™ä¾†æºï¼‰** - æ³¨æ„ï¼šæ­¤è³‡æ–™å¯èƒ½ä¸æº–ç¢ºï¼Œéœ€è¬¹æ…ä½¿ç”¨
3. è­˜åˆ¥å¯èƒ½çš„è‚¡ç¥¨äº¤æ˜“é¡å‹ï¼ˆè²·å…¥buyã€è³£å‡ºsellã€æ’éšŠqueueï¼‰
4. ç‚ºæ¯å€‹æ½›åœ¨äº¤æ˜“è©•ä¼°ç½®ä¿¡åº¦ï¼ˆ0-1åˆ†ï¼‰ï¼š
   - **0.0åˆ†ï¼šå®Œå…¨ä¸ç¢ºå®š** - åªæ˜¯æåŠã€æ²’æœ‰æ˜ç¢ºäº¤æ˜“æ„åœ–
   - **0.5åˆ†ï¼šæœ‰ä¸€å®šè­‰æ“šä½†ä¸å®Œå…¨ç¢ºå®š** - æœ‰äº¤æ˜“è·¡è±¡ä½†è­‰æ“šä¸è¶³
   - **1.0åˆ†ï¼šéå¸¸ç¢ºå®šæœ‰äº¤æ˜“ç™¼ç”Ÿ** - å¤šé …è­‰æ“šæ”¯æŒäº¤æ˜“ç™¼ç”Ÿ
5. æå–æ¯å€‹äº¤æ˜“çš„ç´°ç¯€ï¼ˆè‚¡ç¥¨ä»£è™Ÿã€è‚¡ç¥¨åç¨±ã€æ•¸é‡ã€åƒ¹æ ¼ç­‰ï¼‰

# åˆ†ææ–¹æ³•
- **ä¸»è¦ä¾æ“šï¼šå°è©±è½‰éŒ„å…§å®¹**
  * ç›´æ¥é–±è®€å°è©±å…§å®¹ï¼Œç†è§£ä¸Šä¸‹æ–‡
  * è­˜åˆ¥äº¤æ˜“æ„åœ–çš„é—œéµè©ï¼ˆè²·å…¥ã€è³£å‡ºã€æ’éšŠç­‰ï¼‰
  * æå–æ˜ç¢ºæåˆ°çš„è‚¡ç¥¨ä»£è™Ÿã€åç¨±ã€æ•¸é‡ã€åƒ¹æ ¼
  * ç†è§£å°è©±çš„èªå¢ƒå’ŒçœŸå¯¦æ„åœ–

- **æ¬¡è¦åƒè€ƒï¼šè‚¡ç¥¨åƒè€ƒè³‡æ–™ï¼ˆå¯èƒ½ä¸æº–ç¢ºï¼Œéœ€è¬¹æ…ä½¿ç”¨ï¼‰**
  * stock_number / stock_name: å¾STTè­˜åˆ¥å‡ºçš„è‚¡ç¥¨ä»£è™Ÿå’Œåç¨±
  * corrected_stock_number / corrected_stock_name: ä¿®æ­£å¾Œçš„ä»£è™Ÿå’Œåç¨±
  * original_word: STTè½‰éŒ„çš„åŸå§‹æ–‡å­—
  * relevance_score: è‚¡ç¥¨åœ¨å°è©±ä¸­çš„ç›¸é—œåº¦åˆ†æ•¸ï¼ˆ0-1ï¼‰
  * detection_count: è©²è‚¡ç¥¨è¢«æª¢æ¸¬åˆ°çš„æ¬¡æ•¸
  * detected_by_llms: æª¢æ¸¬åˆ°è©²è‚¡ç¥¨çš„LLMæ¨¡å‹åˆ—è¡¨
  * confidence: æª¢æ¸¬ç½®ä¿¡åº¦ï¼ˆhigh/medium/lowï¼‰
  * **æ³¨æ„ï¼šé€™äº›ä¿¡æ¯åƒ…ä¾›åƒè€ƒï¼Œå„ªå…ˆç›¸ä¿¡å°è©±å…§å®¹æœ¬èº«**

# åˆ¤æ–·æº–å‰‡
- **é¦–å…ˆ**ï¼šä»”ç´°é–±è®€å°è©±å…§å®¹ï¼Œç†è§£çœŸå¯¦æ„åœ–
- **ç„¶å¾Œ**ï¼šåƒè€ƒè‚¡ç¥¨åƒè€ƒè³‡æ–™ä½œç‚ºè¼”åŠ©ï¼Œä½†ä¸è¦å®Œå…¨ä¾è³´
- å¦‚æœå°è©±å…§å®¹èˆ‡åƒè€ƒè³‡æ–™è¡çªï¼Œå„ªå…ˆç›¸ä¿¡å°è©±å…§å®¹
- äº¤å‰é©—è­‰ï¼šå°è©±å…§å®¹ + åƒè€ƒè³‡æ–™çš„å…ƒæ•¸æ“š â†’ æé«˜æº–ç¢ºæ€§
- è¬¹æ…åˆ¤æ–·ï¼šåƒè€ƒè³‡æ–™ä¸­çš„é«˜åˆ†æ•¸ä¸ä¸€å®šä»£è¡¨æœ‰äº¤æ˜“

# ç²µèªè¡“èªå’Œç°¡ç¨±
- è½® = çª©è¼ª
- æ²½/å­¤ = è³£å‡º
- è²·å…¥/å…¥ = è²·å…¥
- æ’éšŠ = æ›å–®ç­‰å¾…æˆäº¤

# è¼¸å‡ºæ ¼å¼
**å¿…é ˆ**è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œåš´æ ¼éµå®ˆä»¥ä¸‹çµæ§‹ï¼š

{
  "transactions": [
    {
      "transaction_type": "buy",  // å¿…é ˆæ˜¯ "buy", "sell", æˆ– "queue"
      "confidence_score": 0.85,   // å¿…é ˆæ˜¯æ•¸å­— 0.0-1.0ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²
      "conversation_number": 1,   // å¿…é ˆæ˜¯æ•´æ•¸ï¼Œè¡¨ç¤ºè©²äº¤æ˜“ä¾†è‡ªå“ªå€‹å°è©±
      "hkt_datetime": "2025-10-20T10:15:30",  // å°è©±çš„æ—¥æœŸæ™‚é–“ï¼ˆç³»çµ±è‡ªå‹•å¾å…ƒæ•¸æ“šæå–ï¼‰
      "broker_id": "B001",        // ç¶“ç´€IDï¼ˆç³»çµ±è‡ªå‹•å¾å…ƒæ•¸æ“šæå–ï¼‰
      "broker_name": "Dickson Lau",  // ç¶“ç´€å§“åï¼ˆç³»çµ±è‡ªå‹•å¾å…ƒæ•¸æ“šæå–ï¼‰
      "client_id": "C123",        // å®¢æˆ¶IDï¼ˆç³»çµ±è‡ªå‹•å¾å…ƒæ•¸æ“šæå–ï¼‰
      "client_name": "CHENG SUK HING",  // å®¢æˆ¶å§“åï¼ˆç³»çµ±è‡ªå‹•å¾å…ƒæ•¸æ“šæå–ï¼‰
      "stock_code": "0700",
      "stock_name": "é¨°è¨Šæ§è‚¡",
      "quantity": "N/A",          // å¦‚æœå¾æ•¸æ“šä¸­ç„¡æ³•ç¢ºå®š
      "price": "N/A",             // å¦‚æœå¾æ•¸æ“šä¸­ç„¡æ³•ç¢ºå®š
      "explanation": "æ ¹æ“šå°è©±å…§å®¹ï¼Œå®¢æˆ¶æ˜ç¢ºæåˆ°'è²·å…¥é¨°è¨Š100æ‰‹'ï¼Œäº¤æ˜“æ„åœ–æ¸…æ™°ã€‚åƒè€ƒè³‡æ–™é¡¯ç¤ºç›¸é—œåº¦0.85ï¼Œæª¢æ¸¬æ¬¡æ•¸3æ¬¡ï¼Œé€²ä¸€æ­¥ç¢ºèªã€‚ç¶œåˆåˆ¤æ–·ç‚ºè²·å…¥äº¤æ˜“ï¼Œç½®ä¿¡åº¦0.85åˆ†"
    }
  ],
  "conversation_analysis": "å°è©±å…§å®¹çš„è©³ç´°åˆ†æ...",
  "overall_summary": "æ•´é«”æ‘˜è¦...ï¼ˆåŸºæ–¼å°è©±å…§å®¹ï¼Œåƒè€ƒè³‡æ–™åƒ…ä½œè¼”åŠ©ï¼‰"
}

**é‡è¦æç¤ºï¼š**
- confidence_score å¿…é ˆæ˜¯æ•¸å­—ï¼ˆfloatï¼‰ï¼Œä¸èƒ½æ˜¯å­—ç¬¦ä¸²
- conversation_number å¿…é ˆæ˜¯æ•´æ•¸ï¼ˆintï¼‰ï¼Œè¡¨ç¤ºè©²äº¤æ˜“ä¾†è‡ªå“ªå€‹å°è©±
- hkt_datetime æœƒè‡ªå‹•å¾å°è©±å…ƒæ•¸æ“šä¸­æå–ä¸¦æ·»åŠ ï¼ˆç³»çµ±æœƒè‡ªå‹•è™•ç†ï¼‰
- broker_id, broker_name, client_id, client_name æœƒè‡ªå‹•å¾å°è©±å…ƒæ•¸æ“šä¸­æå–ä¸¦æ·»åŠ ï¼ˆç³»çµ±æœƒè‡ªå‹•è™•ç†ï¼‰
- explanation å­—æ®µå¿…é ˆè©³ç´°èªªæ˜åˆ¤æ–·ä¾æ“šï¼Œ**å„ªå…ˆå¼•ç”¨å°è©±å…§å®¹**ï¼Œç„¶å¾Œæ‰æ˜¯åƒè€ƒè³‡æ–™
- conversation_analysis å¿…é ˆè©³ç´°åˆ†æå°è©±å…§å®¹
- overall_summary å¿…é ˆåŸºæ–¼å°è©±å…§å®¹ç‚ºä¸»ï¼Œåƒè€ƒè³‡æ–™ç‚ºè¼”
"""


def analyze_transactions_with_json(
    conversation_json_input: str,
    merged_json_input: str,
    model: str,
    ollama_url: str,
    system_message: str,
    temperature: float,
) -> tuple[str, str]:
    """
    Analyze conversation JSON with merged JSON stock data as reference to identify potential transactions
    
    Args:
        conversation_json_input: JSON string with conversation data (primary source)
        merged_json_input: JSON string with merged/deduplicated stock data (reference only, may not be accurate)
        model: LLM model name
        ollama_url: Ollama server URL
        system_message: System prompt for the LLM
        temperature: Temperature parameter
    
    Returns:
        tuple: (summary_result, json_result)
    """
    try:
        # Validate inputs
        if not conversation_json_input or not conversation_json_input.strip():
            error_msg = "âŒ éŒ¯èª¤ï¼šè«‹æä¾›å°è©±JSONæ•¸æ“š"
            return (error_msg, "")
        
        if not model or not model.strip():
            error_msg = "âŒ éŒ¯èª¤ï¼šè«‹æŒ‡å®šæ¨¡å‹åç¨±"
            return (error_msg, "")
        
        if not ollama_url or not ollama_url.strip():
            error_msg = "âŒ éŒ¯èª¤ï¼šè«‹æŒ‡å®š Ollama URL"
            return (error_msg, "")
        
        # Parse conversation JSON to extract conversation text and metadata mapping
        conversation_text = ""
        conversation_info = ""
        conversation_datetime_map = {}  # Map conversation_number -> hkt_datetime
        conversation_broker_id_map = {}  # Map conversation_number -> broker_id
        conversation_broker_name_map = {}  # Map conversation_number -> broker_name
        conversation_client_id_map = {}  # Map conversation_number -> client_id
        conversation_client_name_map = {}  # Map conversation_number -> client_name
        
        try:
            conversations = json.loads(conversation_json_input)
            if not isinstance(conversations, list):
                conversations = [conversations]
            
            conversation_parts = []
            for idx, conv in enumerate(conversations, 1):
                conv_number = conv.get("conversation_number", idx)
                transcriptions = conv.get("transcriptions", {})
                metadata = conv.get("metadata", {})
                
                # Extract metadata fields
                hkt_datetime = metadata.get("hkt_datetime", "N/A")
                broker_id = metadata.get("broker_id", "N/A")
                broker_name = metadata.get("broker_name", "N/A")
                client_id = metadata.get("client_id", "N/A")
                client_name = metadata.get("client_name", "N/A")
                
                conversation_datetime_map[conv_number] = hkt_datetime
                conversation_broker_id_map[conv_number] = broker_id
                conversation_broker_name_map[conv_number] = broker_name
                conversation_client_id_map[conv_number] = client_id
                conversation_client_name_map[conv_number] = client_name
                
                # Extract transcription text
                transcription_text = ""
                if isinstance(transcriptions, dict):
                    for source_name, text in transcriptions.items():
                        if text and text.strip():
                            transcription_text += f"\n[ä¾†æº: {source_name}]\n{text}\n"
                elif isinstance(transcriptions, str):
                    transcription_text = transcriptions
                
                # Format conversation with datetime
                conv_part = f"\n--- å°è©± #{conv_number} ---\n"
                conv_part += f"æ—¥æœŸæ™‚é–“: {hkt_datetime}\n"
                if metadata:
                    conv_part += f"å…ƒæ•¸æ“š: {json.dumps(metadata, ensure_ascii=False)}\n"
                conv_part += f"å…§å®¹:\n{transcription_text}\n"
                conversation_parts.append(conv_part)
            
            conversation_text = "\n".join(conversation_parts)
            conversation_info = f"å…± {len(conversations)} å€‹å°è©±"
            
        except json.JSONDecodeError as e:
            error_msg = f"âŒ éŒ¯èª¤ï¼šç„¡æ³•è§£æå°è©±JSONæ ¼å¼\n\n{str(e)}"
            return (error_msg, "")
        
        # Parse merged JSON to extract stock information (as reference only)
        stock_ref_text = "ï¼ˆç„¡æä¾›ï¼‰"
        stock_list_for_checking = []
        metadata_info = ""
        
        if merged_json_input and merged_json_input.strip():
            try:
                merged_data = json.loads(merged_json_input)
                stocks = merged_data.get("stocks", [])
                metadata = merged_data.get("metadata", {})
                
                # Format metadata if available
                if metadata:
                    metadata_parts = []
                    if "total_conversations" in metadata:
                        metadata_parts.append(f"ç¸½å°è©±æ•¸ï¼š{metadata['total_conversations']}")
                    if "total_analyses" in metadata:
                        metadata_parts.append(f"ç¸½åˆ†ææ•¸ï¼š{metadata['total_analyses']}")
                    if "unique_stocks_found" in metadata:
                        metadata_parts.append(f"å”¯ä¸€è‚¡ç¥¨æ•¸ï¼š{metadata['unique_stocks_found']}")
                    
                    if metadata_parts:
                        metadata_info = f"\n[æ•¸æ“šä¾†æºï¼š{' | '.join(metadata_parts)}]\n"
                
                if stocks:
                    stock_lines = []
                    for idx, stock in enumerate(stocks, 1):
                        # Extract stock information
                        stock_number = stock.get("stock_number", "")
                        stock_name = stock.get("stock_name", "")
                        corrected_number = stock.get("corrected_stock_number", "")
                        corrected_name = stock.get("corrected_stock_name", "")
                        original_word = stock.get("original_word", "")
                        quantity = stock.get("quantity", "")
                        price = stock.get("price", "")
                        relevance = stock.get("relevance_score", 0)
                        confidence = stock.get("confidence", "")
                        detection_count = stock.get("detection_count", 0)
                        detected_by = stock.get("detected_by_llms", [])
                        
                        # Store for explicit checking instruction
                        stock_info = {
                            "original_number": stock_number,
                            "original_name": stock_name,
                            "corrected_number": corrected_number,
                            "corrected_name": corrected_name,
                            "original_word": original_word,
                            "quantity": quantity,
                            "price": price,
                            "relevance": relevance,
                            "confidence": confidence,
                            "detection_count": detection_count,
                            "detected_by": detected_by
                        }
                        stock_list_for_checking.append(stock_info)
                        
                        # Format for display - show comprehensive information
                        line_parts = [f"{idx}."]
                        
                        if stock_number:
                            line_parts.append(f"è‚¡ç¥¨ä»£è™Ÿï¼š{stock_number}")
                        if stock_name:
                            line_parts.append(f"è‚¡ç¥¨åç¨±ï¼š{stock_name}")
                        
                        # Show corrected versions if different
                        if corrected_number and corrected_number != stock_number:
                            line_parts.append(f"[ä¿®æ­£ä»£è™Ÿï¼š{corrected_number}]")
                        if corrected_name and corrected_name != stock_name:
                            line_parts.append(f"[ä¿®æ­£åç¨±ï¼š{corrected_name}]")
                        
                        # Show original word from STT if available
                        if original_word:
                            line_parts.append(f"(åŸæ–‡ï¼š{original_word})")
                        
                        # Show quantity and price if available
                        if quantity:
                            line_parts.append(f"(æ•¸é‡ï¼š{quantity})")
                        if price:
                            line_parts.append(f"(åƒ¹æ ¼ï¼š{price})")
                        
                        # Show relevance score
                        if relevance:
                            line_parts.append(f"(ç›¸é—œåº¦ï¼š{relevance})")
                        
                        # Show confidence
                        if confidence:
                            line_parts.append(f"(ç½®ä¿¡åº¦ï¼š{confidence})")
                        
                        # Show detection count and models
                        if detection_count:
                            line_parts.append(f"(æª¢æ¸¬æ¬¡æ•¸ï¼š{detection_count})")
                        if detected_by:
                            models_str = ", ".join(detected_by)
                            line_parts.append(f"(æª¢æ¸¬æ¨¡å‹ï¼š{models_str})")
                        
                        stock_lines.append("  ".join(line_parts))
                    
                    if stock_lines:
                        stock_ref_text = metadata_info + "\n".join(stock_lines)
            except json.JSONDecodeError as e:
                # If merged JSON is invalid, just ignore it
                stock_ref_text = "ï¼ˆç„¡æ³•è§£æåƒè€ƒè³‡æ–™ï¼‰"
        
        # Build reference note if stock data is available
        reference_note = ""
        if stock_list_for_checking:
            reference_note = f"\n\n**æ³¨æ„ï¼šä»¥ä¸‹è‚¡ç¥¨åƒè€ƒè³‡æ–™åƒ…ä¾›åƒè€ƒï¼Œå¯èƒ½ä¸æº–ç¢ºï¼Œè«‹å„ªå…ˆåˆ†æå°è©±å…§å®¹ï¼š**\n"
            for idx, stock in enumerate(stock_list_for_checking, 1):
                orig_number = stock.get("original_number", "")
                orig_name = stock.get("original_name", "")
                corr_number = stock.get("corrected_number", "")
                corr_name = stock.get("corrected_name", "")
                orig_word = stock.get("original_word", "")
                quantity = stock.get("quantity", "")
                price = stock.get("price", "")
                relevance = stock.get("relevance", 0)
                detection_count = stock.get("detection_count", 0)
                confidence_level = stock.get("confidence", "")
                
                # Build reference item
                ref_items = []
                
                # Stock identification
                if orig_name or orig_number:
                    ref_items.append(f"è‚¡ç¥¨ï¼š{orig_name or ''} ({orig_number or ''})")
                
                # Metadata
                metadata_parts = []
                if relevance:
                    metadata_parts.append(f"ç›¸é—œåº¦={relevance}")
                if detection_count:
                    metadata_parts.append(f"æª¢æ¸¬æ¬¡æ•¸={detection_count}")
                if confidence_level:
                    metadata_parts.append(f"ç½®ä¿¡åº¦={confidence_level}")
                if quantity:
                    metadata_parts.append(f"æ•¸é‡={quantity}")
                if price:
                    metadata_parts.append(f"åƒ¹æ ¼={price}")
                if orig_word:
                    metadata_parts.append(f"åŸæ–‡={orig_word}")
                
                if metadata_parts:
                    ref_items.append(f"[{', '.join(metadata_parts)}]")
                
                if ref_items:
                    reference_note += f"{idx}. {' '.join(ref_items)}\n"
        
        prompt = f"""è«‹ä»”ç´°åˆ†æä»¥ä¸‹å°è©±è½‰éŒ„ï¼Œè­˜åˆ¥æ½›åœ¨çš„è‚¡ç¥¨äº¤æ˜“ã€‚

## ğŸ“ å°è©±è½‰éŒ„å…§å®¹ï¼ˆä¸»è¦è³‡æ–™ä¾†æº - è«‹å„ªå…ˆåˆ†æï¼‰ï¼š
{conversation_text}

{conversation_info}

## ğŸ“Š è‚¡ç¥¨åƒè€ƒè³‡æ–™ï¼ˆæ¬¡è¦è³‡æ–™ä¾†æº - åƒ…ä¾›åƒè€ƒï¼Œå¯èƒ½ä¸æº–ç¢ºï¼‰ï¼š
{stock_ref_text}
{reference_note}

**é‡è¦ä»»å‹™ï¼š**
1. **é¦–å…ˆ**ï¼šä»”ç´°é–±è®€å°è©±è½‰éŒ„å…§å®¹ï¼Œç†è§£å°è©±çš„çœŸå¯¦æ„åœ–å’Œä¸Šä¸‹æ–‡
2. **ç„¶å¾Œ**ï¼šåƒè€ƒè‚¡ç¥¨åƒè€ƒè³‡æ–™ä½œç‚ºè¼”åŠ©ï¼Œä½†ä¸è¦å®Œå…¨ä¾è³´
3. **æ³¨æ„**ï¼šå¦‚æœå°è©±å…§å®¹èˆ‡åƒè€ƒè³‡æ–™è¡çªï¼Œå„ªå…ˆç›¸ä¿¡å°è©±å…§å®¹æœ¬èº«
4. è­˜åˆ¥å°è©±ä¸­çš„äº¤æ˜“æ„åœ–ï¼ˆè²·å…¥/è³£å‡º/æ’éšŠï¼‰
5. æå–äº¤æ˜“ç´°ç¯€ï¼ˆè‚¡ç¥¨ä»£è™Ÿã€è‚¡ç¥¨åç¨±ã€æ•¸é‡ã€åƒ¹æ ¼ï¼‰
6. **å¿…é ˆ**ç‚ºæ¯å€‹äº¤æ˜“æŒ‡å®š conversation_numberï¼ˆå¾å°è©±ç·¨è™Ÿä¸­ç²å–ï¼‰
7. **æ³¨æ„**ï¼šä»¥ä¸‹å­—æ®µæœƒè‡ªå‹•å¾å°è©±å…ƒæ•¸æ“šä¸­æå–ä¸¦æ·»åŠ åˆ°æ¯å€‹äº¤æ˜“ï¼ˆç³»çµ±æœƒè‡ªå‹•è™•ç†ï¼Œä¸éœ€è¦åœ¨è¿”å›çš„JSONä¸­åŒ…å«ï¼‰ï¼š
   - hkt_datetimeï¼ˆæ—¥æœŸæ™‚é–“ï¼‰
   - broker_id, broker_nameï¼ˆç¶“ç´€ä¿¡æ¯ï¼‰
   - client_id, client_nameï¼ˆå®¢æˆ¶ä¿¡æ¯ï¼‰
8. è©•ä¼°ç½®ä¿¡åº¦åˆ†æ•¸ï¼ˆ0.0-1.0ï¼‰ï¼š
   - åŸºæ–¼å°è©±å…§å®¹çš„æ¸…æ™°åº¦
   - åƒè€ƒè³‡æ–™çš„å…ƒæ•¸æ“šå¯ä½œç‚ºè¼”åŠ©åƒè€ƒ
   - ç¶œåˆåˆ¤æ–·çµ¦å‡ºæœ€çµ‚ç½®ä¿¡åº¦
9. åœ¨ conversation_analysis ä¸­è©³ç´°åˆ†æå°è©±å…§å®¹
10. åœ¨ overall_summary ä¸­ç¶œåˆå°è©±å…§å®¹å’Œåƒè€ƒè³‡æ–™çµ¦å‡ºç¸½çµ

è«‹æ ¹æ“šä»¥ä¸Šè³‡æ–™ï¼Œä½¿ç”¨çµæ§‹åŒ–æ ¼å¼è¿”å›åˆ†æçµæœã€‚
"""
        
        # Initialize the LLM with structured output
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
            format="json",  # Request JSON format
        )
        
        # Prepare messages
        messages = [
            ("system", system_message),
            ("human", prompt),
        ]
        
        # Get response
        print(f"ğŸ” Analyzing transactions with {model}...")
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        # Translate LLM response to Traditional Chinese
        response_content = translate_to_traditional_chinese(response_content)
        
        # Try to parse as structured output
        try:
            result_dict = json.loads(response_content)
            
            # Debug: Print raw JSON response
            print("="*60)
            print("ğŸ” DEBUG: Raw LLM JSON Response:")
            print(json.dumps(result_dict, indent=2, ensure_ascii=False))
            print("="*60)
            
            # Extract the structure
            transactions = result_dict.get("transactions", [])
            conversation_analysis = result_dict.get("conversation_analysis", "")
            overall_summary = result_dict.get("overall_summary", "")
            
            # Programmatically add metadata to each transaction based on conversation_number
            for tx in transactions:
                conv_num = tx.get("conversation_number", None)
                if conv_num:
                    tx["hkt_datetime"] = conversation_datetime_map.get(conv_num, "N/A")
                    tx["broker_id"] = conversation_broker_id_map.get(conv_num, "N/A")
                    tx["broker_name"] = conversation_broker_name_map.get(conv_num, "N/A")
                    tx["client_id"] = conversation_client_id_map.get(conv_num, "N/A")
                    tx["client_name"] = conversation_client_name_map.get(conv_num, "N/A")
                else:
                    tx["hkt_datetime"] = "N/A"
                    tx["broker_id"] = "N/A"
                    tx["broker_name"] = "N/A"
                    tx["client_id"] = "N/A"
                    tx["client_name"] = "N/A"
            
            # Count conversations
            try:
                conv_count = len(json.loads(conversation_json_input))
            except:
                conv_count = 1
            
            # Create formatted summary result for all transactions
            summary_result = f"""ğŸ“Š äº¤æ˜“åˆ†æçµæœï¼ˆåŸºæ–¼å°è©±è½‰éŒ„ + è‚¡ç¥¨åƒè€ƒè³‡æ–™ï¼‰
{'='*50}

ğŸ“ åˆ†æçš„å°è©±æ•¸ï¼š{conv_count}
ğŸ“‹ è­˜åˆ¥åˆ°çš„äº¤æ˜“æ•¸ï¼š{len(transactions)}
ğŸ“ˆ åƒè€ƒçš„è‚¡ç¥¨æ•¸ï¼š{len(stock_list_for_checking)}

"""
            
            if len(transactions) == 0:
                summary_result += "â„¹ï¸ æ²’æœ‰è­˜åˆ¥åˆ°ç¢ºå®šçš„äº¤æ˜“\n\n"
            else:
                for idx, tx in enumerate(transactions, 1):
                    tx_type = tx.get("transaction_type", "unknown")
                    tx_conf = tx.get("confidence_score", 0.0)
                    tx_conv_num = tx.get("conversation_number", None)
                    tx_code = tx.get("stock_code", "") or "N/A"
                    tx_name = tx.get("stock_name", "") or "N/A"
                    tx_qty = tx.get("quantity", "") or "N/A"
                    tx_price = tx.get("price", "") or "N/A"
                    tx_exp = tx.get("explanation", "")
                    
                    # Get hkt_datetime from transaction (already added programmatically)
                    tx_datetime = tx.get("hkt_datetime", "N/A")
                    
                    # Transaction type display
                    tx_type_display = {
                        "buy": "è²·å…¥ ğŸ“ˆ",
                        "sell": "è³£å‡º ğŸ“‰",
                        "queue": "æ’éšŠ â³",
                        "unknown": "æœªçŸ¥ â“"
                    }.get(tx_type, tx_type)
                    
                    # Get broker and client info from transaction
                    tx_broker_id = tx.get("broker_id", "N/A")
                    tx_broker_name = tx.get("broker_name", "N/A")
                    tx_client_id = tx.get("client_id", "N/A")
                    tx_client_name = tx.get("client_name", "N/A")
                    
                    summary_result += f"""{'â”€'*50}
äº¤æ˜“ #{idx}
{'â”€'*50}
ğŸ“… æ—¥æœŸæ™‚é–“ (HKT): {tx_datetime}
ğŸ’¬ å°è©±ç·¨è™Ÿ: {tx_conv_num if tx_conv_num else 'N/A'}
ğŸ‘¤ ç¶“ç´€ID: {tx_broker_id}
ğŸ‘” ç¶“ç´€å§“å: {tx_broker_name}
ğŸ†” å®¢æˆ¶ID: {tx_client_id}
ğŸ‘¥ å®¢æˆ¶å§“å: {tx_client_name}
ğŸ”– äº¤æ˜“é¡å‹: {tx_type_display}
â­ ç½®ä¿¡åº¦åˆ†æ•¸: {tx_conf} / 1.0
ğŸ“ˆ è‚¡ç¥¨ä»£è™Ÿ: {tx_code}
ğŸ¢ è‚¡ç¥¨åç¨±: {tx_name}
ğŸ”¢ æ•¸é‡: {tx_qty}
ğŸ’° åƒ¹æ ¼: {tx_price}

ğŸ“ åˆ†æèªªæ˜:
{tx_exp}

"""
            
            if conversation_analysis:
                summary_result += f"""{'='*50}
ğŸ’¬ å°è©±å…§å®¹åˆ†æ:
{conversation_analysis}

"""
            
            summary_result += f"""{'='*50}
ğŸ“„ æ•´é«”æ‘˜è¦:
{overall_summary}
"""
            
            # Format JSON result with proper indentation
            json_result = json.dumps(result_dict, indent=2, ensure_ascii=False)
            
            return (summary_result, json_result)
            
        except json.JSONDecodeError:
            # If not valid JSON, return the raw response
            error_msg = f"âš ï¸ æ¨¡å‹è¿”å›éçµæ§‹åŒ–è¼¸å‡ºï¼š\n\n{response_content}"
            return (error_msg, response_content)
        
    except Exception as e:
        error_msg = f"âŒ éŒ¯èª¤: {str(e)}\n\nè©³ç´°ä¿¡æ¯:\n{traceback.format_exc()}"
        return (error_msg, "")


def create_transaction_analysis_json_tab():
    """Create and return the Transaction Analysis with JSON tab"""
    with gr.Tab("ğŸ“Š Transaction Analysis (JSON)"):
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("#### ğŸ“¥ è¼¸å…¥æ–¹å¼ 1ï¼šå°è©±JSONè¼¸å…¥ï¼ˆä¸»è¦ä¾†æºï¼‰")
                
                conversation_json_box = gr.Textbox(
                    label="å°è©±JSONæ•¸æ“š (Conversation JSON) - ä¸»è¦åˆ†æä¾†æº",
                    placeholder='''[
  {
    "conversation_number": 1,
    "filename": "example.wav",
    "metadata": {
      "broker_id": "B001",
      "broker_name": "Dickson Lau",
      "client_id": "C123",
      "client_name": "CHENG SUK HING",
      "hkt_datetime": "2025-10-20T10:15:30"
    },
    "transcriptions": {
      "sensevoice": "ç¶“ç´€: ä½ å¥½\\nå®¢æˆ¶: æˆ‘æƒ³è²·é¨°è¨Š"
    }
  }
]''',
                    lines=12,
                    show_copy_button=True,
                    info="è²¼ä¸Šå°è©±JSONï¼Œç³»çµ±æœƒè‡ªå‹•æå–è‚¡ç¥¨ä¿¡æ¯å’Œæ—¥æœŸæ™‚é–“"
                )
                
                with gr.Row():
                    conv_llm_checkboxes = gr.CheckboxGroup(
                        choices=LLM_OPTIONS,
                        label="é¸æ“‡LLMé€²è¡Œè‚¡ç¥¨æå–",
                        value=[LLM_OPTIONS[0]],
                        info="é¸æ“‡ä¸€å€‹æˆ–å¤šå€‹LLMä¾†åˆ†æå°è©±"
                    )
                
                with gr.Row():
                    extract_stocks_btn = gr.Button(
                        "ğŸ” å¾å°è©±æå–è‚¡ç¥¨",
                        variant="secondary",
                        size="sm"
                    )
                    use_vector_correction_checkbox = gr.Checkbox(
                        label="ğŸ”§ å•Ÿç”¨å‘é‡æ ¡æ­£",
                        value=True,
                    )
                
                extraction_status_box = gr.Textbox(
                    label="æå–ç‹€æ…‹",
                    lines=4,
                    interactive=False,
                    show_copy_button=True,
                )
                
                gr.Markdown("#### ğŸ“¥ è¼¸å…¥æ–¹å¼ 2ï¼šåˆä½µè‚¡ç¥¨JSONæ•¸æ“šï¼ˆæ¬¡è¦åƒè€ƒï¼‰")
                
                merged_json_box = gr.Textbox(
                    label="åˆä½µè‚¡ç¥¨JSONæ•¸æ“š (Merged JSON) - åƒ…ä½œåƒè€ƒï¼Œå¯èƒ½ä¸æº–ç¢º",
                    placeholder='''{
  "stocks": [
    {
      "stock_number": "00700",
      "stock_name": "é¨°è¨Šæ§è‚¡",
      "relevance_score": 0.85,
      "original_word": "è²·å…¥é¨°è¨Š",
      "corrected_stock_number": "00700",
      "corrected_stock_name": "é¨°è¨Šæ§è‚¡",
      "correction_confidence": 1.0,
      "confidence": "high",
      "detection_count": 3,
      "detected_by_llms": ["qwen2.5:32b", "llama3.1:70b"]
    }
  ],
  "metadata": {
    "total_conversations": 5,
    "total_analyses": 10,
    "unique_stocks_found": 8
  }
}''',
                    lines=10,
                    info="å¾ JSON Batch Analysis çš„ Merged JSON Output è¤‡è£½ï¼Œæˆ–å¾ä¸Šé¢çš„æå–çµæœè‡ªå‹•å¡«å……",
                )
                
                gr.Markdown("#### LLM è¨­å®š")
                
                with gr.Row():
                    model_dropdown = gr.Dropdown(
                        choices=MODEL_OPTIONS,
                        value=DEFAULT_MODEL,
                        label="æ¨¡å‹",
                        allow_custom_value=True,
                    )
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.3,
                        step=0.1,
                        label="Temperature",
                        info="è¼ƒä½çš„æº«åº¦æœƒè®“çµæœæ›´ç¢ºå®š",
                    )
                
                ollama_url_box = gr.Textbox(
                    label="Ollama URL",
                    value=DEFAULT_OLLAMA_URL,
                    placeholder="http://localhost:11434",
                )
                
                system_message_box = gr.Textbox(
                    label="ç³»çµ±è¨Šæ¯ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=6,
                )
                
                analyze_btn = gr.Button(
                    "ğŸš€ é–‹å§‹åˆ†æäº¤æ˜“",
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=1):
                gr.Markdown("#### åˆ†æçµæœ")
                
                # Summary Result Textbox (all transactions)
                summary_result_box = gr.Textbox(
                    label="ğŸ“Š å®Œæ•´çµæœæ‘˜è¦ (All Transactions)",
                    lines=18,
                    interactive=False,
                    show_copy_button=True,
                )
                
                # JSON Result Textbox (raw output)
                json_result_box = gr.Textbox(
                    label="ğŸ“‹ Pydantic JSON è¼¸å‡º (JSON Output)",
                    lines=18,
                    interactive=False,
                    show_copy_button=True,
                )
        
        # Connect the extract stocks button
        extract_stocks_btn.click(
            fn=process_conversation_json_to_merged,
            inputs=[
                conversation_json_box,
                conv_llm_checkboxes,
                ollama_url_box,
                temperature_slider,
                use_vector_correction_checkbox,
            ],
            outputs=[
                extraction_status_box,
                merged_json_box,
            ],
        )
        
        # Connect the analyze button
        analyze_btn.click(
            fn=analyze_transactions_with_json,
            inputs=[
                conversation_json_box,
                merged_json_box,
                model_dropdown,
                ollama_url_box,
                system_message_box,
                temperature_slider,
            ],
            outputs=[
                summary_result_box,
                json_result_box,
            ],
        )

