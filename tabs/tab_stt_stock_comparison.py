"""
Tab: STT & Stock Extraction Comparison
Compare transcriptions from different STT models and extract stock information using multiple LLMs
"""

import traceback
from typing import List, Optional
import gradio as gr
from concurrent.futures import ThreadPoolExecutor, as_completed
from pydantic import BaseModel, Field
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import PydanticOutputParser


# ============================================================================
# Pydantic Models for Structured Output
# ============================================================================

class StockInfo(BaseModel):
    """Information about a single stock mentioned in the conversation"""
    stock_number: str = Field(
        description="The stock code/number (e.g., '00700', '1810', '18138')"
    )
    stock_name: str = Field(
        description="The stock name in Traditional Chinese (e.g., 'È®∞Ë®ä', 'Â∞èÁ±≥', 'ÊãõÂïÜÂ±ÄÁΩÆÂú∞')"
    )
    confidence: str = Field(
        description="Confidence level: 'high', 'medium', or 'low'"
    )
    relevance_score: int = Field(
        description="How sure the conversation talks about this specific stock (0=not discussed, 1=mentioned briefly, 2=actively discussed/traded)"
    )
    reasoning: Optional[str] = Field(
        default=None,
        description="Brief explanation of how the stock was identified or any corrections made"
    )


class ConversationStockExtraction(BaseModel):
    """Complete extraction result from a conversation"""
    stocks: List[StockInfo] = Field(
        description="List of all stocks mentioned in the conversation"
    )
    summary: str = Field(
        description="Brief summary of the conversation context"
    )


# ============================================================================
# Model Configuration
# ============================================================================

LLM_OPTIONS = [
    "qwen3:32b",
    "gpt-oss:20b",
    "gemma3-27b",
    "deepseek-r1:32b",
    "deepseek-r1:70b",
    "qwen2.5:72b",
    "llama3.3:70b",
]

DEFAULT_OLLAMA_URL = "http://localhost:11434"

DEFAULT_SYSTEM_MESSAGE = """‰Ω†ÊòØ‰∏Ä‰ΩçÁ≤æÈÄöÁ≤µË™ûÁöÑÈ¶ôÊ∏ØËÇ°Â∏ÇÂàÜÊûêÂ∞àÂÆ∂„ÄÇ‰Ω†ÁöÑ‰ªªÂãôÊòØÂæûÈõªË©±ÈåÑÈü≥ÁöÑÊñáÂ≠óËΩâÈåÑ‰∏≠Ë≠òÂà•ÊâÄÊúâÊèêÂèäÁöÑËÇ°Á•®„ÄÇ

**ÈáçË¶ÅÊèêÁ§∫:**
- Áî±ÊñºSpeech-to-TextÊäÄË°ìÁöÑË™§Â∑ÆÔºåÊñáÂ≠ó‰∏≠ÂèØËÉΩÊúâË™§Ë™çË©ûÂΩô
- ‰Ω†ÈúÄË¶ÅÈÅãÁî®Â∞àÊ•≠Áü•Ë≠òÂíåÈÇèËºØÊé®ÁêÜÔºåÊé®Êñ∑‰∏¶ÈÇÑÂéüÊ≠£Á¢∫ÁöÑËÇ°Á•®Ë≥áË®ä
- ËÇ°Á•®‰ª£ËôüÂèØËÉΩ‰ª•‰∏çÂêåÂΩ¢ÂºèÂá∫ÁèæÔºà‰æãÂ¶ÇÔºö„Äå‰∏ÉÁôæ„ÄçÂèØËÉΩÊòØ„Äå00700„ÄçÈ®∞Ë®äÔºâ

**Â∏∏Ë¶ãË™§Â∑Æ:**
- Ë™§Ë™ç: Áôæ ‚Üí Ê≠£Á¢∫: ÂÖ´ (‰æã: ‰∏ÄÁôæ‰∏Ä‰∏âÂÖ´ ‚Üí 18138)
- Ë™§Ë™ç: Â≠§/Ê≤Ω ‚Üí Ê≠£Á¢∫: Ë≥£Âá∫
- Ë™§Ë™ç: ËΩÆ ‚Üí Ê≠£Á¢∫: Á™©Ëº™

**‰Ω†ÁöÑÁõÆÊ®ô:**
1. Ë≠òÂà•ÊâÄÊúâÊèêÂèäÁöÑËÇ°Á•®‰ª£ËôüÂíåÂêçÁ®±
2. ‰øÆÊ≠£‰ªª‰ΩïÂèØËÉΩÁöÑSpeech-to-TextË™§Â∑Æ
3. Ë©ï‰º∞ÊØèÂÄãË≠òÂà•ÁöÑÁΩÆ‰ø°Â∫¶Ôºàhigh/medium/lowÔºâ
4. Ë©ï‰º∞Â∞çË©±ËàáË©≤ËÇ°Á•®ÁöÑÁõ∏ÈóúÁ®ãÂ∫¶Ôºàrelevance_scoreÔºâÔºö
   - 0: Ê≤íÊúâÂØ¶Ë≥™Ë®éË´ñÔºàÂÉÖËÉåÊôØÂô™Èü≥ÊàñÁÑ°ÈóúÊèêÂèäÔºâ
   - 1: Á∞°Áü≠ÊèêÂèäÊàñË©¢ÂïèÔºà‰æãÂ¶ÇÔºöÂïèÂÉπ„ÄÅ‰∏ÄËà¨Êü•Ë©¢Ôºâ
   - 2: Á©çÊ•µË®éË´ñÊàñ‰∫§ÊòìÔºà‰æãÂ¶ÇÔºö‰∏ãÂñÆ„ÄÅË©≥Á¥∞ÂàÜÊûê„ÄÅ‰∫§ÊòìÁ¢∫Ë™çÔºâ
5. Êèê‰æõÁ∞°Ë¶ÅÁöÑÊé®ÁêÜËß£Èáã

Ë´ã‰ª•ÁµêÊßãÂåñÁöÑJSONÊ†ºÂºèËøîÂõûÁµêÊûú„ÄÇ"""


# ============================================================================
# Core Extraction Functions
# ============================================================================

def extract_stocks_with_single_llm(
    model: str,
    conversation_text: str,
    system_message: str,
    ollama_url: str,
    temperature: float,
    stt_source: str,
) -> tuple[str, str, str]:
    """
    Extract stock information using a single LLM.
    
    Args:
        model: LLM model name
        conversation_text: The conversation transcript
        system_message: System message for the LLM
        ollama_url: Ollama server URL
        temperature: Temperature for generation
        stt_source: Label for the STT source (for display)
        
    Returns:
        tuple[str, str, str]: (model_name, formatted_result, raw_json)
    """
    try:
        # Initialize parser and LLM
        parser = PydanticOutputParser(pydantic_object=ConversationStockExtraction)
        
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
        )
        
        # Build the complete prompt with format instructions
        format_instructions = parser.get_format_instructions()
        
        full_prompt = f"""{conversation_text}

{format_instructions}

Ë´ãÊåâÁÖß‰∏äËø∞Ê†ºÂºèËøîÂõûÊâÄÊúâË≠òÂà•Âá∫ÁöÑËÇ°Á•®Ë≥áË®ä„ÄÇ"""
        
        # Prepare messages
        messages = [
            ("system", system_message),
            ("human", full_prompt),
        ]
        
        # Get response from LLM
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        # Parse the response
        try:
            parsed_result: ConversationStockExtraction = parser.parse(response_content)
            
            # Format the result for display
            formatted_output = format_extraction_result(parsed_result, model, stt_source)
            
            # Also return the raw JSON for reference
            raw_json = parsed_result.model_dump_json(indent=2, exclude_none=True)
            
            return (model, formatted_output, raw_json)
            
        except Exception as parse_error:
            error_msg = f"‚ö†Ô∏è Warning: Could not parse structured output\n\n"
            error_msg += f"Parse Error: {str(parse_error)}\n\n"
            error_msg += f"Raw LLM Response:\n{response_content}"
            return (model, error_msg, response_content)
    
    except Exception as e:
        error_msg = f"‚ùå Error with {model}: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return (model, error_msg, "")


def format_extraction_result(result: ConversationStockExtraction, model: str, stt_source: str) -> str:
    """Format the extraction result for display"""
    output = []
    
    output.append("=" * 80)
    output.append(f"üìä ËÇ°Á•®ÊèêÂèñÁµêÊûú (Stock Extraction Results)")
    output.append(f"ü§ñ LLM Ê®°Âûã: {model}")
    output.append(f"üé§ STT ‰æÜÊ∫ê: {stt_source}")
    output.append("=" * 80)
    output.append("")
    
    # Summary
    output.append(f"üìù Â∞çË©±ÊëòË¶Å: {result.summary}")
    output.append("")
    
    # Stocks found
    output.append(f"üîç ÊâæÂà∞ {len(result.stocks)} ÂÄãËÇ°Á•®:")
    output.append("")
    
    if not result.stocks:
        output.append("   ‚ö†Ô∏è Êú™ÊâæÂà∞‰ªª‰ΩïËÇ°Á•®Ë≥áË®ä")
    else:
        for i, stock in enumerate(result.stocks, 1):
            confidence_emoji = {
                "high": "‚úÖ",
                "medium": "‚ö°",
                "low": "‚ö†Ô∏è"
            }.get(stock.confidence.lower(), "‚ùì")
            
            relevance_emoji = {
                0: "‚ö´",  # Not discussed
                1: "üîµ",  # Mentioned briefly
                2: "üü¢"   # Actively discussed
            }.get(stock.relevance_score, "‚ùì")
            
            output.append(f"   {i}. {confidence_emoji} ËÇ°Á•® #{i}")
            output.append(f"      ‚Ä¢ ËÇ°Á•®‰ª£Ëôü: {stock.stock_number}")
            output.append(f"      ‚Ä¢ ËÇ°Á•®ÂêçÁ®±: {stock.stock_name}")
            output.append(f"      ‚Ä¢ ÁΩÆ‰ø°Â∫¶: {stock.confidence.upper()}")
            output.append(f"      ‚Ä¢ Áõ∏ÈóúÁ®ãÂ∫¶: {relevance_emoji} {stock.relevance_score}/2")
            
            if stock.reasoning:
                output.append(f"      ‚Ä¢ Êé®ÁêÜ: {stock.reasoning}")
            
            output.append("")
    
    output.append("=" * 80)
    
    return "\n".join(output)


def process_transcriptions(
    transcription1: str,
    transcription2: str,
    selected_llms: list[str],
    system_message: str,
    ollama_url: str,
    temperature: float,
) -> tuple[str, str]:
    """
    Process both transcriptions with selected LLMs and compare results.
    
    Args:
        transcription1: First transcription text
        transcription2: Second transcription text
        selected_llms: List of selected LLM names
        system_message: System message for the LLMs
        ollama_url: Ollama server URL
        temperature: Temperature parameter
        
    Returns:
        tuple[str, str]: (formatted_comparison, raw_json_collection)
    """
    try:
        # Validate inputs
        if not transcription1 or not transcription1.strip():
            return "‚ùå Error: Please provide transcription 1", ""
        
        if not transcription2 or not transcription2.strip():
            return "‚ùå Error: Please provide transcription 2", ""
        
        if not selected_llms or len(selected_llms) == 0:
            return "‚ùå Error: Please select at least one LLM", ""
        
        if not ollama_url or not ollama_url.strip():
            return "‚ùå Error: Please specify Ollama URL", ""
        
        # Results storage
        results_trans1 = {}
        results_trans2 = {}
        raw_jsons = {}
        
        # Process both transcriptions with all selected LLMs concurrently
        with ThreadPoolExecutor(max_workers=len(selected_llms) * 2) as executor:
            futures = {}
            
            # Submit tasks for transcription 1
            for model in selected_llms:
                future = executor.submit(
                    extract_stocks_with_single_llm,
                    model,
                    transcription1,
                    system_message,
                    ollama_url,
                    temperature,
                    "STT Model 1"
                )
                futures[future] = (model, 1)
            
            # Submit tasks for transcription 2
            for model in selected_llms:
                future = executor.submit(
                    extract_stocks_with_single_llm,
                    model,
                    transcription2,
                    system_message,
                    ollama_url,
                    temperature,
                    "STT Model 2"
                )
                futures[future] = (model, 2)
            
            # Collect results as they complete
            for future in as_completed(futures):
                model, trans_num = futures[future]
                result_model, formatted_result, raw_json = future.result()
                
                if trans_num == 1:
                    results_trans1[model] = formatted_result
                    raw_jsons[f"{model}_trans1"] = raw_json
                else:
                    results_trans2[model] = formatted_result
                    raw_jsons[f"{model}_trans2"] = raw_json
        
        # Format output
        output_parts = []
        output_parts.append("=" * 80)
        output_parts.append("üî¨ STT & STOCK EXTRACTION COMPARISON")
        output_parts.append(f"Selected LLMs: {len(selected_llms)}")
        output_parts.append("=" * 80)
        output_parts.append("")
        
        for i, model in enumerate(selected_llms, 1):
            output_parts.append(f"\n{'=' * 80}")
            output_parts.append(f"ü§ñ LLM {i}/{len(selected_llms)}: {model}")
            output_parts.append("=" * 80)
            output_parts.append("")
            
            # Results from transcription 1
            output_parts.append("‚îå‚îÄ üìÑ TRANSCRIPTION 1 RESULTS")
            output_parts.append("‚îÇ")
            result1 = results_trans1.get(model, "‚ùå No response")
            for line in result1.split("\n"):
                output_parts.append(f"‚îÇ  {line}")
            output_parts.append("‚îî" + "‚îÄ" * 79)
            output_parts.append("")
            
            # Results from transcription 2
            output_parts.append("‚îå‚îÄ üìÑ TRANSCRIPTION 2 RESULTS")
            output_parts.append("‚îÇ")
            result2 = results_trans2.get(model, "‚ùå No response")
            for line in result2.split("\n"):
                output_parts.append(f"‚îÇ  {line}")
            output_parts.append("‚îî" + "‚îÄ" * 79)
            output_parts.append("")
        
        output_parts.append("=" * 80)
        output_parts.append("‚úì All comparisons completed")
        output_parts.append("=" * 80)
        
        # Format raw JSON output
        json_output = []
        json_output.append("=" * 80)
        json_output.append("RAW JSON OUTPUTS")
        json_output.append("=" * 80)
        json_output.append("")
        
        for key, value in raw_jsons.items():
            json_output.append(f"\n--- {key} ---")
            json_output.append(value)
            json_output.append("")
        
        return "\n".join(output_parts), "\n".join(json_output)
        
    except Exception as e:
        error_msg = f"‚ùå Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return error_msg, ""


# ============================================================================
# Gradio Tab Creation
# ============================================================================

def create_stt_stock_comparison_tab():
    """Create and return the STT & Stock Comparison tab"""
    with gr.Tab("9Ô∏è‚É£ STT Stock Comparison"):
        gr.Markdown("### Compare Transcriptions & Extract Stock Information")
        gr.Markdown("Input two different transcriptions from different STT models and compare stock extraction results using multiple LLMs.")
        
        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("#### üìù Transcription Inputs")
                
                # Two textboxes for different transcriptions
                transcription1_box = gr.Textbox(
                    label="üé§ Transcription 1 (STT Model 1)",
                    placeholder="Ë´ãËº∏ÂÖ•Á¨¨‰∏ÄÂÄã STT Ê®°ÂûãÁöÑËΩâÈåÑÊñáÊú¨...\n\n‰æãÂ¶ÇÔºö\nÂà∏ÂïÜÔºö‰Ω†Â•ΩÔºåË´ãÂïèÈúÄË¶Å‰ªÄÈ∫ºÂπ´Âä©Ôºü\nÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑È®∞Ë®ä\nÂà∏ÂïÜÔºöÂ•ΩÁöÑÔºå‰∏ÉÁôæËôüÔºåË≤∑Â§öÂ∞ëÔºü",
                    lines=10,
                )
                
                transcription2_box = gr.Textbox(
                    label="üé§ Transcription 2 (STT Model 2)",
                    placeholder="Ë´ãËº∏ÂÖ•Á¨¨‰∫åÂÄã STT Ê®°ÂûãÁöÑËΩâÈåÑÊñáÊú¨...\n\n‰æãÂ¶ÇÔºö\nÂà∏ÂïÜÔºö‰Ω†Â•ΩÔºåË´ãÂïèÈúÄË¶ÅÂí©Âπ´Âä©Ôºü\nÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑È®∞Ë®ä\nÂà∏ÂïÜÔºöÂ•ΩÂòÖÔºå‰∏ÉÁôæËôüÔºåË≤∑ÂπæÂ§öÔºü",
                    lines=10,
                )
                
                gr.Markdown("#### ü§ñ Select LLMs for Analysis")
                
                llm_checkboxes = gr.CheckboxGroup(
                    choices=LLM_OPTIONS,
                    label="Available LLMs",
                    value=[LLM_OPTIONS[0]],  # Default to first model
                    info="Select one or more LLMs to compare their stock extraction results"
                )
                
                gr.Markdown("#### ‚öôÔ∏è Advanced Settings")
                
                system_message_box = gr.Textbox(
                    label="Á≥ªÁµ±Ë®äÊÅØ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=15,
                    max_lines=20,
                    show_copy_button=True,
                )
                
                with gr.Row():
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.1,
                        step=0.1,
                        label="Temperature",
                        info="Lower = more deterministic"
                    )
                
                ollama_url_box = gr.Textbox(
                    label="Ollama URL",
                    value=DEFAULT_OLLAMA_URL,
                    placeholder="http://localhost:11434",
                )
                
                analyze_btn = gr.Button(
                    "üöÄ Analyze & Compare Stock Extraction",
                    variant="primary",
                    size="lg"
                )
            
            with gr.Column(scale=2):
                gr.Markdown("#### üìä Comparison Results")
                
                results_box = gr.Textbox(
                    label="Stock Extraction Comparison",
                    lines=25,
                    interactive=False,
                    show_copy_button=True,
                )
                
                gr.Markdown("#### üîß Raw JSON Outputs")
                
                json_box = gr.Textbox(
                    label="Structured Data (JSON)",
                    lines=12,
                    interactive=False,
                    show_copy_button=True,
                )
        
        # Example buttons
        gr.Markdown("### üí° Á§∫‰æã (Examples)")
        
        with gr.Row():
            example_1 = gr.Button("Á§∫‰æã 1: È®∞Ë®ä‰∫§ÊòìÂ∞çÊØî", size="sm")
            example_2 = gr.Button("Á§∫‰æã 2: Â§öËÇ°Á•® STT Â∑ÆÁï∞", size="sm")
            example_3 = gr.Button("Á§∫‰æã 3: Á≤µË™ûËÆäÈ´îÂ∞çÊØî", size="sm")
        
        # Connect the analyze button
        analyze_btn.click(
            fn=process_transcriptions,
            inputs=[
                transcription1_box,
                transcription2_box,
                llm_checkboxes,
                system_message_box,
                ollama_url_box,
                temperature_slider,
            ],
            outputs=[results_box, json_box],
        )
        
        # Example button handlers
        example_1.click(
            fn=lambda: (
                """Âà∏ÂïÜÔºö‰Ω†Â•ΩÔºåË´ãÂïèÈúÄË¶Å‰ªÄÈ∫ºÂπ´Âä©Ôºü
ÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑È®∞Ë®ä
Âà∏ÂïÜÔºöÂ•ΩÁöÑÔºå‰∏ÉÁôæËôüÈ®∞Ë®äÔºåË≤∑Â§öÂ∞ëÔºü
ÂÆ¢Êà∂Ôºö‰∏ÄÂçÉËÇ°ÔºåÂ∏ÇÂÉπË≤∑ÂÖ•
Âà∏ÂïÜÔºöÁ¢∫Ë™ç‰∏Ä‰∏ãÔºå‰∏ÉÁôæËôüÈ®∞Ë®äÔºåË≤∑ÂÖ•‰∏ÄÂçÉËÇ°ÔºåÂ∏ÇÂÉπÔºåÂ∞çÂóéÔºü
ÂÆ¢Êà∂ÔºöÂ∞çÁöÑÔºåË¨ùË¨ù""",
                """Âà∏ÂïÜÔºö‰Ω†Â•ΩÔºåË´ãÂïèÈúÄË¶ÅÂí©Âπ´Âä©Ôºü
ÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑È®∞Ë®ä
Âà∏ÂïÜÔºöÂ•ΩÂòÖÔºå‰∏ÉÁôæËôüÈ®∞Ë®äÔºåË≤∑ÂπæÂ§öÔºü
ÂÆ¢Êà∂Ôºö‰∏ÄÂçÉËÇ°ÔºåÂ∏ÇÂÉπË≤∑ÂÖ•
Âà∏ÂïÜÔºöÁ¢∫Ë™ç‰∏Ä‰∏ãÔºå‰∏ÉÁôæËôüÈ®∞Ë®äÔºåË≤∑ÂÖ•‰∏ÄÂçÉËÇ°ÔºåÂ∏ÇÂÉπÔºåÂï±ÂîîÂï±Ôºü
ÂÆ¢Êà∂ÔºöÂï±ÔºåË¨ùË¨ù"""
            ),
            outputs=[transcription1_box, transcription2_box],
        )
        
        example_2.click(
            fn=lambda: (
                """ÂÆ¢Êà∂ÔºöÊó©Êô®ÔºåÊàëÊÉ≥Âïè‰∏ãÂ∞èÁ±≥ÂêåÊØî‰∫ûËø™‰ªäÊó•Ëµ∞Âã¢
Âà∏ÂïÜÔºö‰Ω†Â•ΩÔºÅÂ∞èÁ±≥‰∏ÄÂÖ´‰∏ÄÈõ∂‰ªäÊó•ÂçáÂíó2%ÔºåÊØî‰∫ûËø™‰∫å‰∏Ä‰∏Ä‰∏ÄË∑åÂíó1%
ÂÆ¢Êà∂ÔºöÂíÅÊàëÊÉ≥Ê≤Ω‰∫îÁôæËÇ°ÊØî‰∫ûËø™ÔºåÂÜçÂÖ•‰∏ÄÂçÉËÇ°Â∞èÁ±≥
Âà∏ÂïÜÔºöÂ•ΩÁöÑÔºåÁ¢∫Ë™ç‰∏Ä‰∏ãÔºöÊ≤ΩÂá∫ÊØî‰∫ûËø™‰∫å‰∏Ä‰∏Ä‰∏Ä‰∫îÁôæËÇ°ÔºåË≤∑ÂÖ•Â∞èÁ±≥‰∏ÄÂÖ´‰∏ÄÈõ∂‰∏ÄÂçÉËÇ°ÔºåÂï±ÂîîÂï±Ôºü
ÂÆ¢Êà∂ÔºöÂï±ÔºåÂ∞±ÂíÅÂÅö""",
                """ÂÆ¢Êà∂ÔºöÊó©Êô®ÔºåÊàëÊÉ≥Âïè‰∏ãÂ∞èÁ±≥ÂêåÊØî‰∫ûËø™‰ªäÊó•Ëµ∞Âã¢
Âà∏ÂïÜÔºö‰Ω†Â•ΩÔºÅÂ∞èÁ±≥18‰∏ÄÈõ∂‰ªäÊó•ÂçáÂ∑¶2%ÔºåÊØî‰∫ûËø™21‰∏Ä‰∏ÄË∑åÂ∑¶1%
ÂÆ¢Êà∂ÔºöÂíÅÊàëÊÉ≥Ê≤Ω‰∫îÁôæËÇ°ÊØî‰∫ûËø™ÔºåÂÜçÂÖ•‰∏ÄÂçÉËÇ°Â∞èÁ±≥
Âà∏ÂïÜÔºöÂ•ΩÂòÖÔºåÁ¢∫Ë™ç‰∏Ä‰∏ãÔºöÊ≤ΩÂá∫ÊØî‰∫ûËø™21‰∏Ä‰∏Ä‰∫îÁôæËÇ°ÔºåË≤∑ÂÖ•Â∞èÁ±≥18‰∏ÄÈõ∂‰∏ÄÂçÉËÇ°ÔºåÂï±ÂîîÂï±Ôºü
ÂÆ¢Êà∂ÔºöÂï±ÔºåÂ∞±ÂíÅÂÅö"""
            ),
            outputs=[transcription1_box, transcription2_box],
        )
        
        example_3.click(
            fn=lambda: (
                """ÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑ÊãõÂïÜÂ±ÄÁΩÆÂú∞
Âà∏ÂïÜÔºöÊãõÂïÜÂ±ÄÁΩÆÂú∞Ôºå‰øÇ‰∏ÄÁôæ‰∏Ä‰∏âÂÖ´ËôüÔºü
ÂÆ¢Êà∂Ôºö‰øÇÂëÄ
Âà∏ÂïÜÔºöË≤∑ÂπæÂ§öÔºü
ÂÆ¢Êà∂Ôºö‰∫îÁôæËÇ°""",
                """ÂÆ¢Êà∂ÔºöÊàëÊÉ≥Ë≤∑ÊãõÂïÜÂ±ÄÁΩÆÂú∞
Âà∏ÂïÜÔºöÊãõÂïÜÂ±ÄÁΩÆÂú∞Ôºå‰øÇ‰∏ÄÂÖ´‰∏Ä‰∏âÂÖ´ËôüÔºü
ÂÆ¢Êà∂Ôºö‰øÇÂëÄ
Âà∏ÂïÜÔºöË≤∑ÂπæÂ§öÔºü
ÂÆ¢Êà∂Ôºö‰∫îÁôæËÇ°"""
            ),
            outputs=[transcription1_box, transcription2_box],
        )
        
        gr.Markdown(
            """
            ---
            ### üìå ‰ΩøÁî®Ë™™Êòé (Instructions)
            
            1. **Ëº∏ÂÖ•ËΩâÈåÑ**: Â∞áÂÖ©ÂÄã‰∏çÂêå STT Ê®°ÂûãÁöÑËΩâÈåÑÁµêÊûúÂàÜÂà•Ë≤ºÂÖ•ÂÖ©ÂÄãÊñáÊú¨Ê°Ü
            2. **ÈÅ∏Êìá LLM**: ÂãæÈÅ∏‰∏ÄÂÄãÊàñÂ§öÂÄã LLM Ê®°ÂûãÈÄ≤Ë°åËÇ°Á•®ÊèêÂèñÂàÜÊûê
            3. **Ë™øÊï¥Ë®≠ÁΩÆ**: ÂèØÈÅ∏Ë™øÊï¥Á≥ªÁµ±Ë®äÊÅØ„ÄÅTemperature Á≠âÂèÉÊï∏
            4. **ÈñãÂßãÂàÜÊûê**: ÈªûÊìä„ÄåAnalyze & Compare„ÄçÊåâÈàï
            5. **Êü•ÁúãÂ∞çÊØî**: Á≥ªÁµ±ÊúÉ‰∏¶Ë°åËôïÁêÜÊâÄÊúâÁµÑÂêàÔºåÈ°ØÁ§∫Ë©≥Á¥∞Â∞çÊØîÁµêÊûú
            
            ### üéØ ÂäüËÉΩÁâπÈªû (Features)
            
            - üîÑ **ÈõôËΩâÈåÑÂ∞çÊØî**: ÂêåÊôÇËôïÁêÜÂÖ©ÂÄã‰∏çÂêå STT Ê®°ÂûãÁöÑËº∏Âá∫
            - ü§ñ **Â§ö LLM ÂàÜÊûê**: ‰ΩøÁî®Â§öÂÄã LLM Ê®°ÂûãÈÄ≤Ë°å‰∫§ÂèâÈ©óË≠â
            - ‚ö° **‰∏¶Ë°åËôïÁêÜ**: ÊâÄÊúâ LLM Êü•Ë©¢‰∏¶Ë°åÂü∑Ë°åÔºåÊèêÈ´òÊïàÁéá
            - üìä **ÁµêÊßãÂåñËº∏Âá∫**: ‰ΩøÁî® Pydantic ‰øùË≠âÊï∏ÊìöÊ†ºÂºè‰∏ÄËá¥
            - üîç **Êô∫ËÉΩ‰øÆÊ≠£**: Ëá™ÂãïË≠òÂà•‰∏¶‰øÆÊ≠£ STT ÈåØË™§
            - üìà **ÁΩÆ‰ø°Â∫¶Ë©ï‰º∞**: ÊØèÂÄãË≠òÂà•ÁµêÊûúÈÉΩÊúâÁΩÆ‰ø°Â∫¶Ë©ïÂàÜ
            """
        )


