"""
Tab 4: LLM Analysis
Analyze transcriptions using Large Language Models
"""

import csv
import traceback
from pathlib import Path
import gradio as gr

from langchain_ollama import ChatOllama


# Common model options
MODEL_OPTIONS = [
    "qwen3:32b",
    "gpt-oss:20b",
    "gemma3-27b",
    "deepseek-r1:32b",
]

DEFAULT_MODEL = MODEL_OPTIONS[0]
DEFAULT_OLLAMA_URL = "http://localhost:11434"


def load_stock_list(csv_path: str = "stockList.csv") -> str:
    """
    Load stock list from CSV file and format it for the system prompt.
    
    Args:
        csv_path: Path to the stock list CSV file
        
    Returns:
        str: Formatted stock list information
    """
    try:
        stock_list_path = Path(csv_path)
        if not stock_list_path.exists():
            return ""
        
        stock_entries = []
        with open(stock_list_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            next(reader)  # Skip header
            for row in reader:
                if len(row) >= 2:
                    stock_number = row[0].strip()
                    stock_name = row[1].strip()
                    stock_entries.append(f"{stock_number},{stock_name}")
        
        if stock_entries:
            stock_list_text = "\n".join(stock_entries)
            return f"""

## è‚¡ç¥¨ä»£è™Ÿåƒè€ƒåˆ—è¡¨
ä»¥ä¸‹æ˜¯é¦™æ¸¯è‚¡ç¥¨ä»£è™ŸåŠåç¨±åˆ—è¡¨ï¼ˆæ ¼å¼ï¼šè‚¡ç¥¨ä»£è™Ÿ,è‚¡ç¥¨åç¨±ï¼‰ã€‚ä½ å¯ä»¥ä½¿ç”¨æ­¤åˆ—è¡¨ä¾†è­˜åˆ¥å°è©±ä¸­æåˆ°çš„è‚¡ç¥¨åç¨±æˆ–è‚¡ç¥¨ä»£è™Ÿï¼š

{stock_list_text}
"""
        return ""
    except Exception as e:
        print(f"Warning: Could not load stock list: {e}")
        return ""


# Load stock list once at module initialization
STOCK_LIST_INFO = load_stock_list()

DEFAULT_SYSTEM_MESSAGE = (
    f"""
ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªä»¥åŠé¦™æ¸¯è‚¡å¸‚çš„åˆ†æå¸«ã€‚

è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦å¾ä¸‹æ–¹å°è©±ä¸­åˆ¤æ–·èª°æ˜¯åˆ¸å•†ã€èª°æ˜¯å®¢æˆ¶ï¼Œæ•´ç†æœ€çµ‚ä¸‹å–®ï¼ˆè‚¡ç¥¨ä»£è™Ÿã€è²·/è³£ã€åƒ¹æ ¼ã€æ•¸é‡ï¼‰,ä¸‹å–®çš„æ•¸é‡æœ‰å¯èƒ½å¤šæ–¼ä¸€å–®ã€‚

è«‹ç”¨ä¸‹åˆ—çš„è³‡æ–™ä½œåˆ¤åˆ¥æº–å‰‡:

- ç•™æ„å®¢æˆ¶ä¸‹å–®æ™‚å€™ï¼Œåˆ¸å•†ä¸€å®šæœƒå°‡ä¸‹å–®çš„è³‡æ–™é‡è¦†ä¸€æ¬¡è®“å®¢æˆ¶ç¢ºå®šï¼Œè‹¥å°è©±ä¸­æ²’æœ‰ç¢ºå®šçš„å°è©±ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯ä¸‹å–®ã€‚
- å˜—è©¦åœ¨å°è©±æ‰¾å‡ºè‚¡ç¥¨è™Ÿç¢¼å’Œè‚¡ç¥¨åç¨±ï¼Œä¸¦åˆ—å‡ºå¯èƒ½çš„è‚¡ç¥¨è™Ÿç¢¼å’Œè‚¡ç¥¨åç¨±ã€‚
{STOCK_LIST_INFO}
    """
)


def parse_metadata(metadata_text: str) -> dict[str, str]:
    """
    Parse metadata from pasted text format.
    
    Expected format:
        Broker Name: Dickson Lau
        Broker Id: 0489
        Client Number: 97501167
        Client Name: CHAN CHO WING and CHAN MAN LEE
        Client Id: P77751
        UTC: 2025-10-10T01:45:10
        HKT: 2025-10-10T09:45:10
    
    Returns:
        dict: Dictionary with keys: broker_name, broker_id, client_number, 
              client_id, client_name, utc_time, hkt_time
    """
    result = {
        "broker_name": "",
        "broker_id": "",
        "client_number": "",
        "client_id": "",
        "client_name": "",
        "utc_time": "",
        "hkt_time": ""
    }
    
    if not metadata_text or not metadata_text.strip():
        return result
    
    # Parse line by line
    lines = metadata_text.strip().split('\n')
    for line in lines:
        line = line.strip()
        if ':' not in line:
            continue
            
        # Split only on first colon to handle values that contain colons (like times)
        key, value = line.split(':', 1)
        key = key.strip().lower()
        value = value.strip()
        
        # Map the keys to result dictionary
        if 'broker name' in key:
            result['broker_name'] = value
        elif 'broker id' in key:
            result['broker_id'] = value
        elif 'client number' in key:
            result['client_number'] = value
        elif 'client id' in key:
            result['client_id'] = value
        elif 'client name' in key:
            result['client_name'] = value
        elif key == 'utc':
            result['utc_time'] = value
        elif key == 'hkt':
            result['hkt_time'] = value
    
    return result


def analyze_with_llm(
    prompt_text: str,
    prompt_file,
    model: str,
    ollama_url: str,
    system_message: str,
    temperature: float,
    metadata_text: str = "",
) -> str:
    """
    Analyze text with LLM
    
    Returns:
        str: response_text
    """
    try:
        # Determine the prompt source
        final_prompt = None
        
        if prompt_file is not None:
            # Read from uploaded file
            try:
                file_path = Path(prompt_file.name)
                final_prompt = file_path.read_text(encoding="utf-8")
                status = f"âœ“ Loaded prompt from file: {file_path.name}"
            except Exception as e:
                return f"âŒ Error reading file: {str(e)}"
        elif prompt_text and prompt_text.strip():
            # Use text input
            final_prompt = prompt_text.strip()
            status = "âœ“ Using text input"
        else:
            return "âŒ Error: Please provide either text input or upload a file"
        
        # Validate inputs
        if not model or not model.strip():
            return "âŒ Error: Please specify a model name"
        
        if not ollama_url or not ollama_url.strip():
            return "âŒ Error: Please specify Ollama URL"
        
        # Parse metadata from the pasted text
        metadata_dict = parse_metadata(metadata_text)
        
        # Build metadata context if any fields are provided
        metadata_lines = []
        if metadata_dict['broker_name']:
            metadata_lines.append(f"Broker Name: {metadata_dict['broker_name']}")
        if metadata_dict['broker_id']:
            metadata_lines.append(f"Broker Id: {metadata_dict['broker_id']}")
        if metadata_dict['client_number']:
            metadata_lines.append(f"Client Number: {metadata_dict['client_number']}")
        if metadata_dict['client_name']:
            metadata_lines.append(f"Client Name: {metadata_dict['client_name']}")
        if metadata_dict['client_id']:
            metadata_lines.append(f"Client Id: {metadata_dict['client_id']}")
        if metadata_dict['utc_time']:
            metadata_lines.append(f"UTC: {metadata_dict['utc_time']}")
        if metadata_dict['hkt_time']:
            metadata_lines.append(f"HKT: {metadata_dict['hkt_time']}")
        
        # Prepend metadata to system message if available
        final_system_message = system_message
        if metadata_lines:
            metadata_context = "\n".join(metadata_lines)
            final_system_message = f"{system_message}\n\nä»¥ä¸‹æ˜¯å°è©±çš„è³‡æ–™èƒŒæ™¯, å¯èƒ½æœƒå¹«åŠ©ä½ åˆ†æå°è©±:\n{metadata_context}"
        
        print(f"{'*' * 30} final_system_message: {final_system_message} {'*' * 30}")
        
        # Initialize the LLM
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
        )
        
        # Prepare messages
        messages = [
            ("system", final_system_message),
            ("human", final_prompt),
        ]
        
        # Get response
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        return response_content
        
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return error_msg



def create_llm_analysis_tab():
    """Create and return the LLM Analysis tab"""
    with gr.Tab("4ï¸âƒ£ LLM Analysis"):
        gr.Markdown("### Analyze transcriptions using Large Language Models")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("#### Input Settings")
                
                with gr.Tab("æ–‡æœ¬è¼¸å…¥"):
                    llm_prompt_textbox = gr.Textbox(
                        label="å°è©±è¨˜éŒ„",
                        placeholder="",
                        lines=15,
                        value="",
                    )
                
                with gr.Tab("æ–‡ä»¶ä¸Šå‚³"):
                    llm_prompt_file = gr.File(
                        label="ä¸Šå‚³å°è©±è¨˜éŒ„æ–‡ä»¶ (.txt, .json)",
                        file_types=[".txt", ".json"],
                    )
                    gr.Markdown("*ä¸Šå‚³æ–‡ä»¶å°‡å„ªå…ˆæ–¼æ–‡æœ¬è¼¸å…¥*")
                
                gr.Markdown("#### Context Information (Metadata)")
                gr.Markdown("*This information will be included in the system prompt*")
                
                llm_metadata_textbox = gr.Textbox(
                    label="Paste Context Information",
                    placeholder="""""",
                    lines=8,
                    info="Paste all context information at once in the format shown above"
                )
                
                gr.Markdown("#### LLM Settings")
                
                with gr.Row():
                    llm_model_dropdown = gr.Dropdown(
                        choices=MODEL_OPTIONS,
                        value=DEFAULT_MODEL,
                        label="æ¨¡å‹",
                        allow_custom_value=True,
                    )
                    llm_temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature",
                    )
                
                llm_ollama_url = gr.Textbox(
                    label="Ollama URL",
                    value=DEFAULT_OLLAMA_URL,
                    placeholder="http://localhost:11434",
                )
                
                llm_system_message = gr.Textbox(
                    label="ç³»çµ±è¨Šæ¯ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=15,
                )
                
                llm_analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æ", variant="primary", size="lg")
            
            with gr.Column(scale=2):
                gr.Markdown("#### Analysis Results")
                
                llm_response_box = gr.Textbox(
                    label="LLM å›æ‡‰",
                    lines=20,
                    interactive=False,
                )
        
        llm_analyze_btn.click(
            fn=analyze_with_llm,
            inputs=[
                llm_prompt_textbox,
                llm_prompt_file,
                llm_model_dropdown,
                llm_ollama_url,
                llm_system_message,
                llm_temperature_slider,
                llm_metadata_textbox,
            ],
            outputs=[llm_response_box],
        )

