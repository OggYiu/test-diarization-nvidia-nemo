"""
Tab 4: LLM Analysis
Analyze transcriptions using Large Language Models
"""

import csv
import traceback
from pathlib import Path
import gradio as gr

from langchain_ollama import ChatOllama


# Common model options
MODEL_OPTIONS = [
    "qwen3:32b",
    "gpt-oss:20b",
    "gemma3-27b",
    "deepseek-r1:32b",
    "deepseek-r1:70b",
]

DEFAULT_MODEL = MODEL_OPTIONS[0]
DEFAULT_OLLAMA_URL = "http://localhost:11434"

DEFAULT_SYSTEM_MESSAGE = (
    f"""ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªçš„é¦™æ¸¯è‚¡å¸‚çš„åˆ†æå¸«ï¼Œç¾åœ¨ä½ çš„å·¥ä½œæ˜¯é›»è©±éŒ„éŸ³åˆ†æå°ˆå“¡ã€‚ä½ å°‡æœƒåˆ†æé›»è©±éŒ„éŸ³çš„æ–‡å­—ç‰ˆæœ¬ï¼Œä½†ç”±æ–¼Speech To Text æŠ€è¡“çš„èª¤å·®ï¼Œæœ‰å¾ˆå¤šçš„æ–‡å­—æœƒå‡ºç¾èª¤èªä½¿å°è©±å…§å®¹é›£ä»¥ç†è§£ï¼Œè¦ç”¨ä½ çš„è½è°æ˜æ‰æ™ºå»æƒ³åƒåŸæœ¬çš„å°è©±å…§å®¹ã€‚

# è«‹å›æ‡‰ä»¥ä¸‹æœ‰é—œå•é¡Œ
- è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œä¸¦å¾ä¸‹æ–¹å°è©±ä¸­åˆ¤æ–·èª°æ˜¯åˆ¸å•†ã€èª°æ˜¯å®¢æˆ¶ï¼Œæ•´ç†æœ€çµ‚ä¸‹å–®ï¼ˆè‚¡ç¥¨ä»£è™Ÿã€è²·/è³£ã€åƒ¹æ ¼ã€æ•¸é‡ï¼‰ã€‚
- ç•™æ„åœ¨å°è©±ä¸­ï¼Œå¯èƒ½åªæ˜¯åˆ¸å•†è·Ÿå®¢æˆ¶çš„è¨è«–ï¼Œä¸¦æ²’æœ‰ä»»ä½•äº¤æ˜“ï¼Œè‹¥æœ‰äº¤æ˜“ï¼Œä¸‹å–®çš„æ•¸é‡æœ‰å¯èƒ½å¤šæ–¼ä¸€å–®ã€‚

# è«‹ç”¨ä¸‹åˆ—çš„è³‡æ–™ä½œåˆ¤åˆ¥æº–å‰‡
- ç•™æ„å®¢æˆ¶ä¸‹å–®æ™‚å€™ï¼Œåˆ¸å•†ä¸€å®šæœƒå°‡ä¸‹å–®çš„è³‡æ–™é‡è¦†ä¸€æ¬¡è®“å®¢æˆ¶ç¢ºå®šï¼Œè‹¥å°è©±ä¸­æ²’æœ‰ç¢ºå®šçš„å°è©±ï¼Œå¾ˆå¯èƒ½ä¸æ˜¯ä¸‹å–®ã€‚
- å˜—è©¦åœ¨å°è©±æ‰¾å‡ºè‚¡ç¥¨è™Ÿç¢¼å’Œè‚¡ç¥¨åç¨±ï¼Œä¸¦åˆ—å‡ºå¯èƒ½çš„è‚¡ç¥¨è™Ÿç¢¼å’Œè‚¡ç¥¨åç¨±ã€‚

# ä»¥ä¸‹æ˜¯ç°¡ç¨±å’Œè¡“èªï¼Œå¯ä»¥æŸ¥çœ‹ä¸€ä¸‹å»å”åŠ©ä½ äº†è§£å°è©±å…§å®¹:
- ç°¡ç¨±: è½®ï¼Œå…¨å¯«: çª©è¼ª
- ç°¡ç¨±: æ²½/å­¤ï¼Œå…¨å¯«: è³£å‡º

# ä»¥ä¸‹æ˜¯å¸¸è¦‹çš„Speech To Text æŠ€è¡“çš„èª¤å·®:
- èª¤èª: ç™¾ï¼Œæ­£å¯«: å…«ï¼Œä¾‹å­: ä¸€ç™¾ä¸€ä¸‰å…« -> ä¸€å…«ä¸€ä¸‰å…«, å³æ˜¯ 18138
    """

#     """
#     ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªçš„é¦™æ¸¯è‚¡å¸‚åˆ†æå¸«ï¼Œç¾åœ¨ä½ çš„è§’è‰²æ˜¯é›»è©±éŒ„éŸ³åˆ†æå°ˆå“¡ã€‚ä½ å°‡åˆ†æé›»è©±éŒ„éŸ³çš„æ–‡å­—è½‰éŒ„ç‰ˆæœ¬ï¼ˆä¾†è‡ªSpeech-to-TextæŠ€è¡“ï¼‰ï¼Œä½†ç”±æ–¼è½‰éŒ„æŠ€è¡“çš„èª¤å·®ï¼Œæ–‡å­—ä¸­å¯èƒ½å‡ºç¾å¤§é‡èª¤èªè©å½™ï¼Œå°è‡´å°è©±å…§å®¹é›£ä»¥ç†è§£ã€‚ä½ éœ€è¦é‹ç”¨ä½ çš„å°ˆæ¥­çŸ¥è­˜å’Œé‚è¼¯æ¨ç†ï¼Œæ¨æ–·ä¸¦é‚„åŸåŸæœ¬çš„å°è©±æ„åœ–ï¼Œå°¤å…¶æ˜¯æ¶‰åŠè‚¡ç¥¨ç›¸é—œçš„éƒ¨åˆ†ã€‚

# ä½ çš„ä¸»è¦ä»»å‹™æ˜¯ï¼šå¾è½‰éŒ„æ–‡å­—ä¸­æ‰¾å‡ºæ‰€æœ‰æåŠçš„è‚¡ç¥¨åç¨±å’Œè‚¡ç¥¨ä»£ç¢¼ã€‚å¦‚æœæ–‡å­—æœ‰èª¤èªï¼Œè«‹ä¿®æ­£å®ƒå€‘ï¼Œä¸¦è§£é‡‹ä½ çš„æ¨ç†éç¨‹ã€‚æœ€çµ‚è¼¸å‡ºæ‡‰åŒ…æ‹¬ï¼š
# - ä¿®æ­£å¾Œçš„è‚¡ç¥¨åç¨±å’Œä»£ç¢¼åˆ—è¡¨ã€‚
# - å°æ¯å€‹ä¿®æ­£çš„ç°¡è¦è§£é‡‹ã€‚


# ### ä»¥ä¸‹æ˜¯ç°¡ç¨±å’Œè¡“èªï¼Œå¯ä»¥æŸ¥çœ‹ä¸€ä¸‹å»å”åŠ©ä½ äº†è§£å°è©±å…§å®¹:
# - ç°¡ç¨±: è½®ï¼Œå…¨å¯«: çª©è¼ª
# - ç°¡ç¨±: æ²½/å­¤ï¼Œå…¨å¯«: è³£å‡º

# ### å¸¸è¦‹çš„Speech-to-Textèª¤å·®é¡å‹ï¼ˆåŸºæ–¼ç²µèªç™¼éŸ³ç›¸ä¼¼æ€§ï¼‰ï¼š
# - èª¤èªç¤ºä¾‹ï¼šå°‡ã€Œå…«ã€èª¤èªçˆ²ã€Œç™¾ã€ã€‚  
#   ä¾‹å­ï¼šè½‰éŒ„æ–‡å­—çˆ²ã€Œä¸€ç™¾ç™¾ä¸ƒã€ï¼Œæ­£ç¢ºæ‡‰çˆ²ã€Œä¸€å…«å…«ä¸ƒã€ï¼Œå³è‚¡ç¥¨ä»£ç¢¼1887ã€‚

# åœ¨åˆ†ææ™‚ï¼Œè«‹è€ƒæ…®ä¸Šä¸‹æ–‡æ¨æ–·å¯èƒ½çš„ä¿®æ­£ã€‚å¦‚æœç„¡æ³•ç¢ºå®šï¼Œè«‹æ¨™è¨»çˆ²ã€Œä¸ç¢ºå®šã€ä¸¦æä¾›å‚™é¸å¯èƒ½æ€§ã€‚
# """
)


def parse_metadata(metadata_text: str) -> dict[str, str]:
    """
    Parse metadata from pasted text format.
    
    Expected format:
        Broker Name: Dickson Lau
        Broker Id: 0489
        Client Number: 97501167
        Client Name: CHAN CHO WING and CHAN MAN LEE
        Client Id: P77751
        UTC: 2025-10-10T01:45:10
        HKT: 2025-10-10T09:45:10
    
    Returns:
        dict: Dictionary with keys: broker_name, broker_id, client_number, 
              client_id, client_name, utc_time, hkt_time
    """
    result = {
        "broker_name": "",
        "broker_id": "",
        "client_number": "",
        "client_id": "",
        "client_name": "",
        "utc_time": "",
        "hkt_time": ""
    }
    
    if not metadata_text or not metadata_text.strip():
        return result
    
    # Parse line by line
    lines = metadata_text.strip().split('\n')
    for line in lines:
        line = line.strip()
        if ':' not in line:
            continue
            
        # Split only on first colon to handle values that contain colons (like times)
        key, value = line.split(':', 1)
        key = key.strip().lower()
        value = value.strip()
        
        # Map the keys to result dictionary
        if 'broker name' in key:
            result['broker_name'] = value
        elif 'broker id' in key:
            result['broker_id'] = value
        elif 'client number' in key:
            result['client_number'] = value
        elif 'client id' in key:
            result['client_id'] = value
        elif 'client name' in key:
            result['client_name'] = value
        elif key == 'utc':
            result['utc_time'] = value
        elif key == 'hkt':
            result['hkt_time'] = value
    
    return result


def analyze_with_llm(
    prompt_text: str,
    prompt_file,
    model: str,
    ollama_url: str,
    system_message: str,
    temperature: float,
    metadata_text: str = "",
) -> str:
    """
    Analyze text with LLM
    
    Returns:
        str: response_text
    """
    try:
        # Determine the prompt source
        final_prompt = None
        
        if prompt_file is not None:
            # Read from uploaded file
            try:
                file_path = Path(prompt_file.name)
                final_prompt = file_path.read_text(encoding="utf-8")
                status = f"âœ“ Loaded prompt from file: {file_path.name}"
            except Exception as e:
                return f"âŒ Error reading file: {str(e)}"
        elif prompt_text and prompt_text.strip():
            # Use text input
            final_prompt = prompt_text.strip()
            status = "âœ“ Using text input"
        else:
            return "âŒ Error: Please provide either text input or upload a file"
        
        # Validate inputs
        if not model or not model.strip():
            return "âŒ Error: Please specify a model name"
        
        if not ollama_url or not ollama_url.strip():
            return "âŒ Error: Please specify Ollama URL"
        
        # Parse metadata from the pasted text
        metadata_dict = parse_metadata(metadata_text)
        
        # Build metadata context if any fields are provided
        metadata_lines = []
        if metadata_dict['broker_name']:
            metadata_lines.append(f"Broker Name: {metadata_dict['broker_name']}")
        if metadata_dict['broker_id']:
            metadata_lines.append(f"Broker Id: {metadata_dict['broker_id']}")
        if metadata_dict['client_number']:
            metadata_lines.append(f"Client Number: {metadata_dict['client_number']}")
        if metadata_dict['client_name']:
            metadata_lines.append(f"Client Name: {metadata_dict['client_name']}")
        if metadata_dict['client_id']:
            metadata_lines.append(f"Client Id: {metadata_dict['client_id']}")
        if metadata_dict['utc_time']:
            metadata_lines.append(f"UTC: {metadata_dict['utc_time']}")
        if metadata_dict['hkt_time']:
            metadata_lines.append(f"HKT: {metadata_dict['hkt_time']}")
        
        # Prepend metadata to system message if available
        final_system_message = system_message
        if metadata_lines:
            metadata_context = "\n".join(metadata_lines)
            final_system_message = f"{system_message}\n\nä»¥ä¸‹æ˜¯å°è©±çš„è³‡æ–™èƒŒæ™¯, å¯èƒ½æœƒå¹«åŠ©ä½ åˆ†æå°è©±:\n{metadata_context}"
        
        print(f"{'*' * 30} final_system_message: {final_system_message} {'*' * 30}")
        
        # Initialize the LLM
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
        )
        
        # Prepare messages
        messages = [
            ("system", final_system_message),
            ("human", final_prompt),
        ]
        
        # Get response
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        return response_content
        
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return error_msg



def create_llm_analysis_tab():
    """Create and return the LLM Analysis tab"""
    with gr.Tab("4ï¸âƒ£ LLM Analysis"):
        gr.Markdown("### Analyze transcriptions using Large Language Models")
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("#### Input Settings")
                
                with gr.Tab("æ–‡æœ¬è¼¸å…¥"):
                    llm_prompt_textbox = gr.Textbox(
                        label="å°è©±è¨˜éŒ„",
                        placeholder="",
                        lines=15,
                        value="",
                    )
                
                with gr.Tab("æ–‡ä»¶ä¸Šå‚³"):
                    llm_prompt_file = gr.File(
                        label="ä¸Šå‚³å°è©±è¨˜éŒ„æ–‡ä»¶ (.txt, .json)",
                        file_types=[".txt", ".json"],
                    )
                    gr.Markdown("*ä¸Šå‚³æ–‡ä»¶å°‡å„ªå…ˆæ–¼æ–‡æœ¬è¼¸å…¥*")
                
                gr.Markdown("#### Context Information (Metadata)")
                gr.Markdown("*This information will be included in the system prompt*")
                
                llm_metadata_textbox = gr.Textbox(
                    label="Paste Context Information",
                    placeholder="""""",
                    lines=8,
                    info="Paste all context information at once in the format shown above"
                )
                
                gr.Markdown("#### LLM Settings")
                
                with gr.Row():
                    llm_model_dropdown = gr.Dropdown(
                        choices=MODEL_OPTIONS,
                        value=DEFAULT_MODEL,
                        label="æ¨¡å‹",
                        allow_custom_value=True,
                    )
                    llm_temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature",
                    )
                
                llm_ollama_url = gr.Textbox(
                    label="Ollama URL",
                    value=DEFAULT_OLLAMA_URL,
                    placeholder="http://localhost:11434",
                )
                
                llm_system_message = gr.Textbox(
                    label="ç³»çµ±è¨Šæ¯ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=15,
                )
                
                llm_analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æ", variant="primary", size="lg")
            
            with gr.Column(scale=2):
                gr.Markdown("#### Analysis Results")
                
                llm_response_box = gr.Textbox(
                    label="LLM å›æ‡‰",
                    lines=20,
                    interactive=False,
                )
        
        llm_analyze_btn.click(
            fn=analyze_with_llm,
            inputs=[
                llm_prompt_textbox,
                llm_prompt_file,
                llm_model_dropdown,
                llm_ollama_url,
                llm_system_message,
                llm_temperature_slider,
                llm_metadata_textbox,
            ],
            outputs=[llm_response_box],
        )

