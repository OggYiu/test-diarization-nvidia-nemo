# Quick Start Guide

Get started with the audio processing pipeline in 5 minutes!

## Prerequisites

- Python 3.8 or higher
- FFmpeg installed on your system

## Installation

### Step 1: Install PyTorch

```bash
# CPU version (recommended for testing)
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cpu

# OR GPU version (for production)
# pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### Step 2: Install dependencies

```bash
pip install -r requirements.txt
```

This will install:
- NeMo Toolkit (for diarization)
- FunASR (for speech-to-text)
- LangChain Ollama (for LLM analysis)
- Other required packages

**Note**: First run will download several GB of models (NeMo, SenseVoice). Be patient!

## Quick Test

### Test without LLM (no Ollama required)

```bash
python audio_pipeline.py path/to/your/audio.wav --skip-llm
```

This will:
1. âœ… Diarize the audio (identify speakers)
2. âœ… Chop audio into speaker segments
3. âœ… Transcribe each segment
4. âœ… Save all results to `./output/`

### Test with LLM (requires Ollama)

First, install and start Ollama:

```bash
# Install Ollama from https://ollama.ai
# Then pull a model
ollama pull qwen2.5:7b-instruct

# Start Ollama server
ollama serve
```

Then run the pipeline:

```bash
python audio_pipeline.py path/to/your/audio.wav \
  --llm-model qwen2.5:7b-instruct \
  --ollama-url http://localhost:11434
```

## Output

Check the `./output/` directory:

```
output/
â”œâ”€â”€ transcriptions/
â”‚   â”œâ”€â”€ conversation.txt      â† Read this! (formatted conversation)
â”‚   â””â”€â”€ transcriptions.json   â† Full details
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ analysis.txt          â† LLM analysis (if not skipped)
â”œâ”€â”€ audio_chunks/             â† Segmented audio files
â””â”€â”€ diarization/              â† Diarization results
```

## Common Options

```bash
# Process English audio
python audio_pipeline.py audio.wav --language en

# Specify number of speakers
python audio_pipeline.py audio.wav --num-speakers 3

# Change output directory
python audio_pipeline.py audio.wav --work-dir ./my_results

# Use different STT model (larger, more accurate)
python audio_pipeline.py audio.wav --stt-model iic/SenseVoiceLarge
```

## Troubleshooting

### "Module 'nemo' not found"
```bash
pip install nemo_toolkit[all]
```

### "FFmpeg not found"
- Ubuntu/Debian: `sudo apt-get install ffmpeg`
- macOS: `brew install ffmpeg`
- Windows: Download from [ffmpeg.org](https://ffmpeg.org/download.html)

### "Ollama connection error"
- Make sure Ollama is running: `ollama serve`
- Or skip LLM: `--skip-llm`

### "CUDA out of memory"
- Use CPU version of PyTorch
- Or reduce batch size in the config

## Next Steps

- Read the full [README.md](README.md) for detailed documentation
- Customize the LLM prompt with `--prompt`
- Process multiple files with a bash script
- Integrate into your own application

## Example for Hong Kong Stock Trading Calls

```bash
python audio_pipeline.py cantonese_call.wav \
  --language yue \
  --num-speakers 2 \
  --llm-model qwen2.5:7b-instruct \
  --prompt "ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªä»¥åŠé¦™æ¸¯è‚¡å¸‚çš„åˆ†æå¸«ã€‚è«‹å¾å°è©±ä¸­æ‰¾å‡ºè²·è³£çš„è‚¡ç¥¨ä»£è™Ÿã€åƒ¹æ ¼ã€æ•¸é‡ç­‰è³‡è¨Šã€‚"
```

Happy processing! ğŸ‰

