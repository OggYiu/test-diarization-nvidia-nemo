# ğŸš€ Stock Extractor - Quick Start Guide

## What I Created For You

I've created a **Stock Information Extractor** that uses:
- âœ… **Pydantic** for structured data validation
- âœ… **Multiple LLM models** (you can choose which one to use)
- âœ… **Gradio** for an easy-to-use web interface

## Files Created

1. **`stock_extractor_gui.py`** - Main application with Gradio UI
2. **`test_stock_extractor.py`** - Test script to try it without the GUI
3. **`STOCK_EXTRACTOR_README.md`** - Detailed documentation
4. **`STOCK_EXTRACTOR_QUICKSTART.md`** - This file

## Quick Start (3 Steps)

### Step 1: Install Dependencies

```bash
pip install pydantic langchain-core langchain-ollama gradio
```

Or if you want to update all requirements:
```bash
pip install -r requirements.txt
```

### Step 2: Make Sure Ollama is Running

```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not, start it
ollama serve

# Make sure you have at least one model installed
ollama pull qwen3:32b
```

### Step 3: Run the Application

**Option A: Launch the Gradio GUI**
```bash
python stock_extractor_gui.py
```
Then open your browser to: http://localhost:7860

**Option B: Test from Command Line**
```bash
python test_stock_extractor.py simple
```

## How It Works

### Input (Conversation):
```
åˆ¸å•†ï¼šä½ å¥½ï¼Œè«‹å•éœ€è¦ä»€éº¼å¹«åŠ©ï¼Ÿ
å®¢æˆ¶ï¼šæˆ‘æƒ³è²·é¨°è¨Š
åˆ¸å•†ï¼šå¥½çš„ï¼Œä¸ƒç™¾è™Ÿé¨°è¨Šï¼Œè²·å¤šå°‘ï¼Ÿ
å®¢æˆ¶ï¼šä¸€åƒè‚¡ï¼Œå¸‚åƒ¹è²·å…¥
```

### Output (Structured JSON via Pydantic):
```json
{
  "stocks": [
    {
      "stock_number": "00700",
      "stock_name": "é¨°è¨Š",
      "confidence": "high",
      "reasoning": "å°è©±ä¸­æåˆ°ã€Œä¸ƒç™¾è™Ÿã€ï¼Œå°æ‡‰é¦™æ¸¯è‚¡ç¥¨ä»£ç¢¼00700"
    }
  ],
  "summary": "å®¢æˆ¶è©¢å•è³¼è²·é¨°è¨Šè‚¡ç¥¨"
}
```

## Key Features

### 1. Pydantic Models
The script uses two Pydantic models to ensure structured output:

```python
class StockInfo(BaseModel):
    stock_number: str      # e.g., "00700"
    stock_name: str        # e.g., "é¨°è¨Š"
    confidence: str        # "high", "medium", "low"
    reasoning: Optional[str]  # Explanation

class ConversationStockExtraction(BaseModel):
    stocks: List[StockInfo]
    summary: str
```

This guarantees:
- âœ… Consistent data structure
- âœ… Type validation
- âœ… Easy to parse and use programmatically
- âœ… Can be directly saved to database or exported

### 2. Multiple LLM Model Selection
Choose from these models (or add your own):
- `qwen3:32b` â­ (Recommended for Chinese)
- `deepseek-r1:32b` (Good reasoning)
- `deepseek-r1:70b` (Better quality)
- `gpt-oss:20b`
- `gemma3-27b`
- `qwen2.5:72b`
- `llama3.3:70b`

### 3. Gradio Interface
- ğŸ¨ Beautiful, modern UI
- ğŸ“ Text input with examples
- âš™ï¸ Configurable settings (model, temperature, system message)
- ğŸ“Š Formatted output + raw JSON
- ğŸ’¡ Pre-loaded example conversations

## Testing Examples

### Test Simple Conversation
```bash
python test_stock_extractor.py simple
```

### Test with Multiple Stocks
```bash
python test_stock_extractor.py multiple_stocks
```

### Test with STT Errors (e.g., "ä¸€ç™¾ä¸€ä¸‰å…«" â†’ should be "18138")
```bash
python test_stock_extractor.py with_errors
```

### Test All Conversations
```bash
python test_stock_extractor.py all
```

### Compare Different Models
```bash
python test_stock_extractor.py compare
```

### Test with Specific Model
```bash
python test_stock_extractor.py simple deepseek-r1:32b
```

## Using in Your Code

You can import and use the function directly:

```python
from stock_extractor_gui import (
    extract_stocks_from_conversation,
    DEFAULT_SYSTEM_MESSAGE,
    ConversationStockExtraction
)
import json

conversation = """
åˆ¸å•†ï¼šä½ å¥½
å®¢æˆ¶ï¼šæˆ‘æƒ³è²·é¨°è¨Šå’Œå°ç±³
"""

# Get the extraction
result_text, json_str = extract_stocks_from_conversation(
    conversation_text=conversation,
    model="qwen3:32b",
    ollama_url="http://localhost:11434",
    system_message=DEFAULT_SYSTEM_MESSAGE,
    temperature=0.1,
)

# Parse JSON to Python dict
data = json.loads(json_str)

# Access structured data
for stock in data["stocks"]:
    print(f"Stock: {stock['stock_name']} ({stock['stock_number']})")
    print(f"Confidence: {stock['confidence']}")
```

## Gradio Interface Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Stock Information Extractor                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ Input                    â”‚  ğŸ“Š Output                        â”‚
â”‚                              â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Conversation Text      â”‚ â”‚  â”‚ Formatted Results            â”‚â”‚
â”‚  â”‚                        â”‚ â”‚  â”‚                              â”‚â”‚
â”‚  â”‚                        â”‚ â”‚  â”‚ â€¢ Stock #1: é¨°è¨Š (00700)     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚   Confidence: HIGH           â”‚â”‚
â”‚                              â”‚  â”‚                              â”‚â”‚
â”‚  âš™ï¸ Model: qwen3:32b       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚  ğŸŒ¡ï¸ Temperature: 0.1       â”‚                                   â”‚
â”‚  ğŸ”— Ollama: localhost:11434 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                              â”‚  â”‚ Raw JSON Output              â”‚â”‚
â”‚  [ğŸš€ é–‹å§‹æå–]              â”‚  â”‚ { "stocks": [...] }          â”‚â”‚
â”‚                              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Configuration Options

### Temperature Setting
- **0.0 - 0.3**: More deterministic, factual (recommended for extraction)
- **0.4 - 0.7**: Balanced
- **0.8 - 2.0**: More creative, varied output

### System Message
You can customize the system message to:
- Add more examples of common errors
- Include specific terminology for your use case
- Add rules for edge cases

### Model Selection
- **For accuracy**: Use larger models (qwen2.5:72b, deepseek-r1:70b)
- **For speed**: Use smaller models (qwen3:32b, deepseek-r1:32b)
- **For Cantonese**: qwen models work best

## Common Use Cases

### 1. Post-STT Processing
```
Audio â†’ Speech-to-Text â†’ Stock Extractor â†’ Database
```

### 2. Trade Verification
```
Conversation â†’ Extract Stocks â†’ Cross-check with trades.csv
```

### 3. Compliance Checking
```
Call Recording â†’ Extract â†’ Verify against order records
```

### 4. Data Analysis
```
Multiple calls â†’ Extract all stocks â†’ Analyze trading patterns
```

## Troubleshooting Quick Tips

| Issue | Solution |
|-------|----------|
| Connection refused | Run `ollama serve` |
| Model not found | Run `ollama pull qwen3:32b` |
| Parsing errors | Lower temperature to 0.1 |
| Slow responses | Use smaller model (qwen3:32b) |
| Wrong extractions | Add more context to system message |

## Next Steps

1. âœ… **Try the examples** - Run the test script
2. âœ… **Launch the GUI** - See the Gradio interface
3. âœ… **Customize** - Adjust system message for your needs
4. âœ… **Integrate** - Use in your existing pipeline
5. âœ… **Experiment** - Try different models and compare results

## Questions?

- ğŸ“– Read the full documentation: `STOCK_EXTRACTOR_README.md`
- ğŸ§ª Run tests: `python test_stock_extractor.py all`
- ğŸ¨ Launch GUI: `python stock_extractor_gui.py`

---

**Happy Extracting! ğŸ‰**

