"""
Gradio GUI for LLM Analysis using Ollama
Analyzes phone call transcriptions using LangChain and Ollama
"""

import gradio as gr
from langchain_ollama import ChatOllama
from pathlib import Path
import traceback

# Common model options
MODEL_OPTIONS = [
    "qwen2.5vl:32b",
    "gpt-oss:20b",
    "qwen3:30b",
    "gemma3-27b",
]

# Default configuration
DEFAULT_MODEL = MODEL_OPTIONS[0]
DEFAULT_OLLAMA_URL = "http://192.168.61.2:11434"
DEFAULT_SYSTEM_MESSAGE = (
    "ä½ æ˜¯ä¸€ä½ç²¾é€šç²µèªä»¥åŠé¦™æ¸¯è‚¡å¸‚çš„åˆ†æå¸«ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡å›æ‡‰ï¼Œ"
    "ä¸¦å¾ä¸‹æ–¹å°è©±ä¸­åˆ¤æ–·èª°æ˜¯åˆ¸å•†ã€èª°æ˜¯å®¢æˆ¶ï¼Œæ•´ç†æœ€çµ‚ä¸‹å–®ï¼ˆè‚¡ç¥¨ä»£è™Ÿã€è²·/è³£ã€åƒ¹æ ¼ã€æ•¸é‡ï¼‰ï¼Œ"
)


def analyze_with_llm(
    prompt_text: str,
    prompt_file,
    model: str,
    ollama_url: str,
    system_message: str,
    temperature: float,
) -> tuple[str, str]:
    """
    Analyze text with LLM
    
    Returns:
        tuple: (status_message, response_text)
    """
    try:
        # Determine the prompt source
        final_prompt = None
        
        if prompt_file is not None:
            # Read from uploaded file
            try:
                file_path = Path(prompt_file.name)
                final_prompt = file_path.read_text(encoding="utf-8")
                status = f"âœ“ Loaded prompt from file: {file_path.name}"
            except Exception as e:
                return f"âŒ Error reading file: {str(e)}", ""
        elif prompt_text and prompt_text.strip():
            # Use text input
            final_prompt = prompt_text.strip()
            status = "âœ“ Using text input"
        else:
            return "âŒ Error: Please provide either text input or upload a file", ""
        
        # Validate inputs
        if not model or not model.strip():
            return "âŒ Error: Please specify a model name", ""
        
        if not ollama_url or not ollama_url.strip():
            return "âŒ Error: Please specify Ollama URL", ""
        
        # Initialize the LLM
        status += f"\nâœ“ Connecting to Ollama at: {ollama_url}"
        status += f"\nâœ“ Using model: {model}"
        status += f"\nâœ“ Temperature: {temperature}"
        
        chat_llm = ChatOllama(
            model=model,
            base_url=ollama_url,
            temperature=temperature,
        )
        
        # Prepare messages
        messages = [
            ("system", system_message),
            ("human", final_prompt),
        ]
        
        status += "\nâœ“ Sending request to LLM..."
        
        # Get response
        resp = chat_llm.invoke(messages)
        
        # Extract content
        try:
            response_content = getattr(resp, "content", str(resp))
        except Exception:
            response_content = str(resp)
        
        status += "\nâœ“ Analysis complete!"
        
        return status, response_content
        
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}\n\nTraceback:\n{traceback.format_exc()}"
        return error_msg, ""


def load_example_file():
    """Load an example transcription file if available"""
    example_paths = [
        Path("demo/transcriptions/conversation.txt"),
        Path("output/transcriptions/conversation.txt"),
    ]
    
    for path in example_paths:
        if path.exists():
            return path.read_text(encoding="utf-8")
    
    return "è«‹è¼¸å…¥é›»è©±å°è©±è¨˜éŒ„æ–‡æœ¬ï¼Œæˆ–ä¸Šå‚³æ–‡ä»¶ã€‚"


def create_ui():
    """Create the Gradio interface"""
    
    with gr.Blocks(title="LLM Phone Call Analyzer", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸ“ LLM Phone Call Analyzer
            ä½¿ç”¨ Ollama å’Œ LangChain åˆ†æé›»è©±å°è©±è¨˜éŒ„
            """
        )
        
        with gr.Row():
            with gr.Column(scale=2):
                gr.Markdown("### è¼¸å…¥è¨­å®š")
                
                # Prompt input
                with gr.Tab("æ–‡æœ¬è¼¸å…¥"):
                    prompt_textbox = gr.Textbox(
                        label="å°è©±è¨˜éŒ„",
                        placeholder="è«‹è¼¸å…¥æˆ–ç²˜è²¼é›»è©±å°è©±è¨˜éŒ„...",
                        lines=15,
                        value=load_example_file(),
                    )
                
                with gr.Tab("æ–‡ä»¶ä¸Šå‚³"):
                    prompt_file = gr.File(
                        label="ä¸Šå‚³å°è©±è¨˜éŒ„æ–‡ä»¶ (.txt, .json)",
                        file_types=[".txt", ".json"],
                    )
                    gr.Markdown("*ä¸Šå‚³æ–‡ä»¶å°‡å„ªå…ˆæ–¼æ–‡æœ¬è¼¸å…¥*")
                
                # Configuration
                gr.Markdown("### LLM è¨­å®š")
                
                with gr.Row():
                    model_dropdown = gr.Dropdown(
                        choices=MODEL_OPTIONS,
                        value=DEFAULT_MODEL,
                        label="æ¨¡å‹",
                        allow_custom_value=True,
                    )
                    temperature_slider = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature",
                    )
                
                ollama_url = gr.Textbox(
                    label="Ollama URL",
                    value=DEFAULT_OLLAMA_URL,
                    placeholder="http://localhost:11434",
                )
                
                system_message = gr.Textbox(
                    label="ç³»çµ±è¨Šæ¯ (System Message)",
                    value=DEFAULT_SYSTEM_MESSAGE,
                    lines=3,
                )
                
                # Action button
                analyze_btn = gr.Button("ğŸš€ é–‹å§‹åˆ†æ", variant="primary", size="lg")
            
            with gr.Column(scale=2):
                gr.Markdown("### åˆ†æçµæœ")
                
                status_box = gr.Textbox(
                    label="ç‹€æ…‹",
                    lines=6,
                    interactive=False,
                )
                
                response_box = gr.Textbox(
                    label="LLM å›æ‡‰",
                    lines=20,
                    interactive=False,
                )
        
        # Examples
        gr.Markdown("### ğŸ’¡ æç¤º")
        gr.Markdown(
            """
            - æ”¯æŒå¾æ–‡æœ¬æ¡†ç›´æ¥è¼¸å…¥æˆ–ä¸Šå‚³ .txt/.json æ–‡ä»¶
            - ç³»çµ±è¨Šæ¯å¯ä»¥è‡ªå®šç¾©ä»¥é©æ‡‰ä¸åŒçš„åˆ†æéœ€æ±‚
            - Temperature è¶Šé«˜ï¼Œå›æ‡‰è¶Šæœ‰å‰µæ„ï¼›è¶Šä½ï¼Œå›æ‡‰è¶Šç¢ºå®š
            - ç¢ºä¿ Ollama æœå‹™æ­£åœ¨é‹è¡Œä¸¦ä¸”å¯ä»¥è¨ªå•
            """
        )
        
        # Connect the button
        analyze_btn.click(
            fn=analyze_with_llm,
            inputs=[
                prompt_textbox,
                prompt_file,
                model_dropdown,
                ollama_url,
                system_message,
                temperature_slider,
            ],
            outputs=[status_box, response_box],
        )
    
    return demo


def main():
    """Launch the Gradio app"""
    demo = create_ui()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7860,
        share=False,
        inbrowser=True,
    )


if __name__ == "__main__":
    main()

